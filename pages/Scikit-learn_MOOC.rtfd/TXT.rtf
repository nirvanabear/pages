{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Baskerville;\f1\fnil\fcharset0 AndaleMono;\f2\fnil\fcharset0 AppleColorEmoji;
\f3\fnil\fcharset0 Baskerville-Bold;\f4\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red21\green21\blue19;\red255\green255\blue255;\red14\green14\blue12;
}
{\*\expandedcolortbl;;\cssrgb\c10532\c10494\c9396;\cssrgb\c100000\c100000\c100000;\cssrgb\c6707\c6587\c5056;
}
\margl1440\margr1440\vieww13600\viewh7060\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs30 \cf2 \cb3 \
\

\f1 \cf2 import sklearn\
sklearn.show_versions()
\f0 \cf2 \
	\'95 Displays the versions of the dependencies for scikit-learn\
\

\f1 \cf2 pydoc modules
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97\cf2 \ul \ulc4 MODULE 0: INTRODUCTION TO MACHINE LEARNING CONCEPTS
\f0\b0 \cf2 \ulnone \

\f3\b \cf2 \
\'a7 0.1.1 \cf2 \ul \ulc4 What is Machine Learning?\cf2 \ulnone \
	
\f0\b0 \cf2 \'95 https://www.youtube.com/watch?v=f0b11x2tAZw
\f3\b \cf2 \
	
\f0\b0 \cf2 \'95 Identifying Irises\
		\'ac Describe flowers with numbers and identify with mathematical rules\
		\'ac petal size and sepal size \
	\'95 Identifying rich people from census data\
	\
	\'95\'a0Rule creation is automated and built from the data\
	\'95 Predictive analysis\
		\'ac\'a0relies on statistical tools, but go beyond standard stats\
		\'ac many sournces of variability\
		\'ac noise\
			\uc0\u9674  a performance review may be affected by the mood of the boss...\
	\'95\'a0Memorizing\
		\'ac given a new individual, predict based on the closest match that's already in the database\
		\'ac\'a0nearest neighbor predictor\
		\'ac error rate\
			\uc0\u9674  test set: all data that we're examining is within the data set\
	\'95\'a0Generalizing\
		\uc0\u8800  to Memorizing\
		test data \uc0\u8800  training data\
		\'ac now use that model on data that has not been used to develop the model\
		\'ac error will increase\
	\'95\'a0Data matrix\
		\'ac samples\
			\uc0\u9674  rows are observations \
		\'ac features\
			\uc0\u9674 \'a0columns are descriptors \
	\'95 Supervised machine learning\
		\'ac data matrix, \
			\uc0\u9674  X, \
			\uc0\u9674  with n samples\
			\uc0\u9674  and m features\
		\'ac target\
			\uc0\u9674  y, \
			\uc0\u9674  a property of each observation\
		\'ac goal is to predict y\
	\'95\'a0Unsupervised learning\
		\'ac No labels\
			\uc0\u9674  not covered in class for now\
		\'ac Algorithm finds trends on its own\
		\'ac matrix, X, \
		\'ac\'a0no available target\
		\'ac extract some goal from the structure or patterns in the data\
			\uc0\u9674  not covered\
	\'95 Regression and classificaiton\
		\'ac Classification\
			\uc0\u9674  y is discrete\
			\uc0\u9674  what kind of iris: setosa, versicolor, virginica\
		\'ac Regression\
			\uc0\u9674  y is continuous, numerical quantity\
			\uc0\u9674  wage prediciton\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97\cf2 \ul \ulc4 MODULE 1.1: TABULAR DATA EXPLORATION
\f0\b0 \cf2 \ulnone \
		\

\f3\b \cf2 \ul \ulc4 Predictive Modeling Pipeline\cf2 \ulnone \
	\
	
\f0\b0 \cf2 \'95 Preparing the data for analysis:\
		\'ac Loading the data\
		\'ac Differentiating the variables of the data set\
			\uc0\u9674  Numerical\
			\uc0\u9674  Categorical\
		\'ac Visualizing the distribution of the variables\

\f3\b \cf2 \
\'a7 1.1.1 \cf2 \ul \ulc4 First look at our dataset
\f0\b0 \cf2 \ulnone \
		\'95 01_tabular_data_exploration.ipynb\
		\'95 Loading the adult census dataset\
			\'ac CSV file\
			\'ac pandas will read it into the kernel\
				
\f1 \cf2 adult_census = pd.read_csv("../datasets/adult-census.csv")
\f0 \cf2 \
			\'ac Goal:\
				\uc0\u9674  predict yes or no:\
					\'86 does someone earn more than 50K\
					\'86 based on\
						\'bb age\
						\'bb emmployment\
						\'bb education\
						\'bb family status\
		\'95 The variables (columns) in the dataset\
			\'ac Pandas dataframe\
				\uc0\u9674  where imported data is stored\
				\uc0\u9674  two dimensions\
					\'86 rows:	samples\
					\'86 columns:	features\
\
			\'ac 
\f3\b \cf2 X\
				\uc0\u9674  
\f0\b0 \cf2 data\
				\uc0\u9674  columns in data\
					\'86 feature\
					\'86 variable\
					\'86 attribute\
					\'86 covariate\
				\uc0\u9674  rows in data\
					\'86 sample\
					\'86 record\
					\'86 instance\
					\'86 observation\
			\'ac 
\f3\b \cf2 y
\f0\b0 \cf2 \
				\uc0\u9674  column in data\
					\'86 target\
		\
			\'ac Class imbalance\
				\uc0\u9674  more or fewer samples in one class than the others\
				\uc0\u9674  may require special techniques\
			\'ac Defining columns (variables) as numerical or categorical:\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2640\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth9860\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth500\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 numerical_columns = [\
    "age", "education-num", "capital-gain", "capital-loss",\
    "hours-per-week"]\
categorical_columns = [\
    "workclass", "education", "marital-status", "occupation",\
    "relationship", "race", "sex", "native-country"]\
all_columns = numerical_columns + categorical_columns + [target_column]\
\
adult_census = adult_census[all_columns]
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\'ac Check the size of samples and features:\
				
\f1 \cf2 adult_census.shape[1]\

\f0 \cf2 					\'86 but subtract one, because one of the columns is the target
\f1 \cf2 \
				adult_census.shape[0]
\f0 \cf2 \
				\
		
\f3\b \cf2 \'95\'a0Visual inspection of the data
\f0\b0 \cf2 \
			\'ac Look for:\
				\uc0\u9674  can it be solved without machine learning?\
				\uc0\u9674 \'a0is the required information in the dataset?\
				\uc0\u9674  peculiarities: missing data, malfunctioning sensor, capped values\
\
			\'ac Quick look:\
				
\f1 \cf2 adult_census.head()\
			
\f0 \cf2 \'ac Plot a histogram\
				
\f1 \cf2 _ = adult_census.hist(figsize=(20, 14))\

\f0 \cf2 				\uc0\u9674  Numerical data distrubution\
					\'86 The underscore variable is used by convention as a garbage variable\
				\uc0\u9674  Observations:\
					\'86 Min or max values along x-axis:\
						\'bb eg: few points over 70 (retired people filtered out)\
					\'86 Peak values or zeros of y-axis\
						\'bb eg: education-num peaks twice at 10 and again at 13 \
						\'bb eg: hours per week peaks at 40\
						\'bb eg: capital gain and loss both have one peak then near zero elsewhere\
			\'ac Display counts\
				
\f1 \cf2 adult_census["sex"].value_counts()
\f0 \cf2 \
				\uc0\u9674  Categorical variable distribution\
					\'86 eg: gender or education level counts\
			\'ac Compare two variables directly\
				
\f1 \cf2 pd.crosstab(index=adult_census["education"],\
 					columns=adult_census["education-num"])\

\f0 \cf2 				\uc0\u9674  checking for duplicate data\
					\'86 if the data shows up in one spot for each column and row\
					\'86 variables with duplicate data should be eliminated\
						\'bb ML has problems with duplicated or highly correclated data\
			\'ac Compare two variables against the target\
				
\f1 \cf2 seaborn.pairplot(...)
\f0 \cf2 \
				\uc0\u9674  interactions between the different variables\
					\'86 plots along the diagonal just show regular histograms of the data\
				
\f1 \cf2 seaborn.scatterplot(...)
\f0 \cf2 \
					\'86 shows just a single plot\
				\uc0\u9674  Find regions which contain mostly one class\
					\'86 sketch in some rough handdrawn boundaries\
					\'86 decision trees make similar divisions in regions for analysis\
			\'ac Limitations with ML\
				\uc0\u9674  imbalanced number of samples in a target variable category\
				\uc0\u9674  redundant or highly correlated columns (variables)\
				\uc0\u9674  linear models only accurately predict linear relationships\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97\cf2 \ul \ulc4 MODULE 1.2: FITTING A SCIKIT-LEARN MODEL ON NUMERICAL DATA
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 1.2.1 \cf2 \ul \ulc4 First model with scikit-learn
\f0\b0 \cf2 \ulnone \
	\'95 02_numerical_pipeline_introduction.ipynb\
	\'95 Loading the dataset and check\
		
\f1 \cf2 adult_census = pd.read_csv("../datasets/adult-census-numeric.csv")
\f0 \cf2 \
		
\f1 \cf2 adult_census.head()
\f0 \cf2 \
	\'95 Separate the data and the target\
		\'ac Columns can be dropped as a list of names\
			
\f1 \cf2 columns=[target_name, ]
\f0 \cf2 \
		\'ac Check number of rows\
			
\f1 \cf2 data.shape[0]
\f0 \cf2 \
	\'95 K-nearest neighbors\
		
\f1 \cf2 from sklearn.neighbors import KNeighborsClassifier\
		model = KNeighborsClassifier()
\f0 \cf2 \
	\'95 Train the model\
		
\f1 \cf2 model.fit(data, target)
\f0 \cf2 \
			\uc0\u9674  fit has two elements\
				\'86 learning algorithm\
					\'bb takes training data and target as input\
				\'86 some model states\
					\'bb used to predict (or transform)\
	\'95 Data and target\
		\'ac X and y\
	\'95 Predict\
		
\f1 \cf2 target_predicted = model.predict(data)
\f0 \cf2 \
	\'95 Display predictions and actual (target)\
		
\f1 \cf2 target_predicted[:5]\
		target[:5]
\f0 \cf2 \
	\'95 Compare predictions to actual\
		
\f1 \cf2 target[:5] == target_predicted[:5]
\f0 \cf2 \
			\uc0\u9674  displays subset of true or false\
	\'95 Compare total percentage correct\
		
\f1 \cf2 (target == target_predicted).mean()
\f0 \cf2 \
	\

\f3\b \cf2 	\'b6 Ideas and concepts
\f0\b0 \cf2 \
		\'95 How to build model with:\
			\'ac tabular datasets\
			\'ac only numerical features\
		
\f3\b \cf2 \'95 Train-test data split
\f0\b0 \cf2 \
			\'ac However, data used to build the model was also used to test\
				\uc0\u9674  using data separated from training data, better assessment of prediction accuracy\
				\uc0\u9674  separated data: data_test and target_test\
			
\f1 \cf2 accuracy = model.score(data_test, target_test)
\f0 \cf2 \
		\'95 Predictive Model Assessment:\
			\'ac 
\f3\b \cf2 generalization performance
\f0\b0 \cf2 \
				\uc0\u9674  also:\
					\'86 
\f3\b \cf2 predictive performance
\f0\b0 \cf2 \
					\'86 
\f3\b \cf2 statistical performance
\f0\b0 \cf2 \
				\uc0\u9674  test score or test error\
				\uc0\u9674  comparing predicted to true targets\
			\'ac 
\f3\b \cf2 computational performance
\f0\b0 \cf2 \
				\uc0\u9674  assess computational costs of training or predicting when using a certain model\
\
\

\f3\b \cf2 	\'b6 Excercise M1.02
\f0\b0 \cf2 \
		\'95 02_numerical_pipeline_cross_ex_00.ipynb\
		
\f1 \cf2 sklearn.neighbors.KNeighborsClassifier
\f0 \cf2 \
			\uc0\u9674  rarely used in practice, but intuitive to understand\
			\uc0\u9674  all scikit-learn models can be created without arguments\
				\'86 simplicity and accessibility in ML\
		\'ac Scikit-learn\
			
\f1 \cf2 .fit(X, y)		
\f0 \cf2 trainingax2 = sns.scatterplot(data=true_v_predict_ridge, x="measured_power", y="predicted_power",\
                     color="black", alpha=0.5)\
\
xpoints = ypoints = plt.xlim()\
ax2.plot(xpoints, ypoints, color="blue", lw=3, alpha=.5)\
_ = ax2.set_title(f"Ridge: measured vs. predicted")\
			
\f1 \cf2 .predict(X)		
\f0 \cf2 predictions\
			
\f1 \cf2 .score(X, y)	
\f0 \cf2 evaluate a model\
		\'ac Documentation in a notebook\
			
\f1 \cf2 KNeighborsClassifier?
\f0 \cf2 \
		\
\

\f3\b \cf2 \'a7 1.2.3 \cf2 \ul \ulc4 Working with numerical data
\f0\b0 \cf2 \ulnone \
	\'95 02_numerical_pipeline_hands_on.ipynb\
	\'95 Identifying numerical data in the dataset\
		\'ac 
\f1 \cf2 df.dtypes
\f0 \cf2 \
	\'95  Separating numerical data out\
		
\f1 \cf2 numerical_columns = \
			["age", "capital-gain", "capital-loss", "hours-per-week"]\
		data[numerical_columns].head()\
	\'95
\f0 \cf2  Data range for a certain attribute\
		\'ac 
\f1 \cf2 df['column'].describe()\
	
\f0 \cf2 \'95 Train/test split helper function\
		
\f1 \cf2 data_train, data_test, target_train, target_test = 
\f0 \cf2 \
		
\f1 \cf2 sklearn.model_selection.train_test_split(\
				data_numeric, target, random_state=42, test_size=0.25)
\f0 \cf2 \
					\'bb function returns a list of four outputs \
			
\f1 \cf2 random_state
\f0 \cf2  \
				\'86 
\f3\b \cf2 deterministic randomization
\f0\b0 \cf2  \
					\'bb repeatability\
					\'bb but accuracy is dependent upon the particular split (see cross-validation)\
			
\f1 \cf2 test_size
\f0 \cf2  \
				\'86 chooses how much of the data to set aside for testing\
				\'86 eg: the above code takes 25% of the samples for the test set\
			
\f1 \cf2 shuffle=False
\f0 \cf2 \
				\'86 eg: chooses the last 25% of the set for the test data\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 	
\f0 \cf2 \'95 Logistic Regression\
		
\f1 \cf2 from sklearn import set_config\
		set_config(display='diagram')  # to display nice model diagram
\f0 \cf2 \
		
\f1 \cf2 model = sklearn.linear_model.LogisticRegression()
\f0 \cf2 \
			\uc0\u9674  more commonly used in production than K-nearest...\
	\'95 Train and evaluate the model\
		
\f1 \cf2 model.fit(data_train, target_train)\
		model.score(data_test, target_test)
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 1.2.4 \cf2 \ul \ulc4 Exercise M1.03: Dummy Classifier
\f0\b0 \cf2 \ulnone \
	\'95 02_numerical_pipeline_ex01.ipynb\
	\'95 Creates a model (like K-nearest or linear regression)\
		\'ac However, this model will predict random or constant outcomes		\'ac Type is set by 
\f1 \cf2 strategy
\f0 \cf2 \
	\'95 Dummy Classifier\
		
\f1 \cf2 dumdum = DummyClassifier(strategy="constant", constant=" >50K")
\f0 \cf2 \
	\'95 Model is then run past methods fit and score\
		
\f1 \cf2 dumdum.fit(data_train, target_train)\
		dumdum.score(data_test, target_test)
\f0 \cf2 \
			\uc0\u9674  
\f3\b \cf2 baseline classifier
\f0\b0 \cf2 \
				\'86 gives reference to answer question: is our statistical model accuracy good?\

\f3\b \cf2 \
\'a7 1.2.5 \cf2 \ul \ulc4 Preprocessing for numerical features
\f0\b0 \cf2 \ulnone \
	\'95 02_numerical_pipeline_scaling.ipynb\
	\'95\'a0 
\f3\b \cf2 Preprocessing
\f0\b0 \cf2 \
		\'ac Adjusting the dataset to standardize the data\
		\'ac 
\f3\b \cf2 scaling 
\f0\b0 \cf2   /   
\f3\b \cf2 standardizationn
\f0\b0 \cf2 \
			\uc0\u9674  a dataset's different columns usually span different ranges\
			\uc0\u9674  
\f3\b \cf2 normalization
\f0\b0 \cf2 \
				\'86 a transformer shifts and scales each feature to:\
					\'bb mean = 0\
					\'bb a unit standard deviation\
		\'ac\'a0Different models benefit differently from scaling\
	\'95 Normalizing transformer\
		
\f1 \cf2 scaler = sklearn.preprocessing.StandardScalar()
\f0 \cf2 \
			\uc0\u9674  beneficial to k-Nearest Neighbors and Logistic Regression\
			\uc0\u9674  not useful for Decision Trees, but not harmful either\
			\uc0\u9674  Returns arrays containing means and SDs for each column\
		
\f1 \cf2 scaler.fit(data_train)
\f0 \cf2 \
			\uc0\u9674  uses only a single argument: Data/X\
			\uc0\u9674  no target column\
			\uc0\u9674  generates object attributes learned from data\
			\uc0\u9674  end in underscore\
				
\f1 \cf2 mean_		
\f0 \cf2 array of means for each column (attribute, feature)
\f1 \cf2 \
				scale_		
\f0 \cf2 array of standard deviations for each column\
		
\f1 \cf2 data_train_scaled = scaler.transform(data_train)
\f0 \cf2 \
			\uc0\u9674  returns a new data set\
				\'86 a numpy array, not a dataframe\
			\uc0\u9674  data transformed into scaled data\
				\'86 smilar to 
\f1 \cf2 .predict 
\f0 \cf2 step with predictors\
					\'86 just that it uses a function and takes the model state and data as input\
				\'86 outputs transformed data (2-d array) instead of predictions (1-d array)\
		
\f1 \cf2 data_train_scaled = scaler.fit_transform(data_train)
\f0 \cf2 \
			\uc0\u9674  performs both actions in one step \
	\'95 Reformat as dataframe and relabel columns\
		
\f1 \cf2 data_train_scaled = pd.DataFrame(data_train_scaled,\
                                 columns=data_train.columns)
\f0 \cf2 \
	\'95 Visual display of how StandardScalar changes the data:\
		
\f1 \cf2 num_points_to_plot = 300
\f0 \cf2 \
		
\f1 \cf2 sns.jointplot(data=data_train[:num_points_to_plot], x="age",\
              y="hours-per-week", marginal_kws=dict(bins=15))\
		sns.jointplot(data=data_train[:num_points_to_plot], x="age",\
              y="hours-per-week", marginal_kws=dict(bins=15))
\f0 \cf2 \
				\uc0\u9674  Distribution is the same, but scale of the axes is different\
	\'95 Make a scaling pipeline\
		
\f1 \cf2 from sklearn.pipeline import make_pipeline
\f0 \cf2 \
		
\f1 \cf2 model = make_pipeline(StandardScaler(), LogisticRegression())
\f0 \cf2 \
			\uc0\u9674  automatically names each object, eg: 
\f1 \cf2 standardscaler\
			model.named_steps
\f0 \cf2 \
				\'86 chains together operations\
				\'86 from library 
\f1 \cf2 sklearn.pipeline
\f0 \cf2 \
				\'86 Returns a model with the same methods (.fit, .predict, .score...)\
	\'95 Calling fit on a pipeline scales and trains a model\
		
\f1 \cf2 model.fit(data_train, target_train)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf2 			\uc0\u9674  Executes three methods all together:			Argument:\
				
\f1 \cf2 transformer.fit()				data_train\
				transformer.transform()			data_train\
				predictor.fit()					data_train, target_train\
\

\f0 \cf2 	\'95 Then do some predictions on some test data\
		
\f1 \cf2 predicted_target = model.predict(data_test)\
			
\f0 \cf2 \uc0\u9674  Executes two methods:\
				
\f1 \cf2 transformer.transform()\
				predictor.predict()
\f0 \cf2 \
			\uc0\u9674  Test data must be scaled before predictions can be performed\
	\'95 Reasons to scale:\
		\'ac Non-scaled data warning\
			\uc0\u9674  using non-scaled data could cause the required number of iterations to exceed 
\f1 \cf2 max_iter
\f0 \cf2 \
			\uc0\u9674  this would leave an unfinished result as the answer\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 1.2.6 \cf2 \ul \ulc4 Model evaluation using Cross-validation
\f0\b0 \cf2 \ulnone \
	\'95 02_numerical_pipeline_cross_validation.ipynb\
	\'95 Why use cross-validation?\
		\'ac small data sets produce very small training or test sets\
		\'ac greater confidence in the result\
	\'95 Why not?\
		\'ac Computationally expensive\
	\'95 Cross-validation does multiple splits, and multiple models, and aggregates the results		\'ac estimates variability in the statistical performance\
	\'95 K-fold\
		\'ac one of several cross-validation strategies\
			\uc0\u9674  dataset is split into K-partitions\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2021-05-31 at 8.40.46 AM.png \width10640 \height4340 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	\'95 Cross-validated, scaled linear regression:\
		
\f1 \cf2 model = make_pipeline(StandardScaler(), LogisticRegression())\
		cv_result = cross_validate(model, data_numeric, target, cv=5)
\f0 \cf2 \
			\uc0\u9674  parameter 
\f1 \cf2 cv
\f0 \cf2  defines the number of splits\
			\uc0\u9674  takes untrained model, attributes and target sets as the other arguments\
			\uc0\u9674  Returns a dictionary, with entries for each fold\
				\'86 training time\
				\'86 prediction time\
				\'86 score for each fold\
	\'95 By default, does NOT produce a model\
		\'ac Estimates the generalization performance\
			\uc0\u9674  trained with the full training set\
		\'ac Estimates the variability\
			\uc0\u9674  ie: uncertainty of the generalization accuracy\
	\'95 Average accuracy is:\
		
\f1 \cf2 scores = cv_result["test_score"]\
		f"\{scores.mean():.3f\} +/- \{scores.std():.3f\}"
\f0 \cf2 			\uc0\u9674  mean with uncertainty (standard deviation)\
	\'95 Standard deviation estimates model uncertainty\
		\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97 \cf2 \ul \ulc4 MODULE 1.3: HANDLING CATEGORICAL DATA
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 1.3.1 \cf2 \ul \ulc4 Encoding of categorical variables
\f0\b0 \cf2 \ulnone \
	\'95 03_categorical_pipeline.ipynb\
	\'95 Identify categorical variables\
		
\f1 \cf2 data.dtypes
\f0 \cf2 \
			\'ac eg: int64 or object\
	\'95 Select features based on data type\
		
\f1 \cf2 sklearn.compose.make_column_selector(dtype_include=object)\
		categorical_columns = categorical_columns_selector(data)
\f0 \cf2 \
			\uc0\u9674  selects columns based on their type\
			\uc0\u9674  Returns an Object which can then takes datasets as arguments\
				\'bb which then returns a list of column labels\
		
\f1 \cf2 data_categorical = data[categorical_columns]
\f0 \cf2 \
			\uc0\u9674  removes numeric types\
\
	
\f3\b \cf2 \'b6 Strategies to encode categories\
		
\f0\b0 \cf2 \'95 Encoding ordinal categories\
			
\f1 \cf2 encoder = sklearn.preprocessing.OrdinalEncoder()\
			data_encoded = encoder.fit_transform(data_categorical)
\f0 \cf2 \
				
\f1 \cf2 .fit
\f0 \cf2 			Analyzes categories to determine numerical equivalents\
				
\f1 \cf2 .transform		
\f0 \cf2 Applies the new numerical labels to the data\
				\uc0\u9674  Numpy array\
				\uc0\u9674  number of output columns matches columns inputted\
			
\f1 \cf2 encoder.categories_
\f0 \cf2 \
				\uc0\u9674  this attribute shows the mapping between categories and numerical value\
			\'ac Lexicographical mapping strategy\
				\uc0\u9674  numerical order follows alphabetical order\
					\'86 eg: size labels would be ordered: \
						L < M < S < XL\
			\'ac 
\f1 \cf2 categories
\f0 \cf2  \
				\uc0\u9674  constructor argument allows for explicit designation\
			\'ac Caution:\
				\uc0\u9674  Predictive models tend to assume values are ordered\
					\'86 ie: 0 < 1 < 2 < 3...\
				\uc0\u9674   if no ordering actually exists, eg, countries, one-hot encoding is preferred\
		\'95 Encoding nominal categories\
			
\f1 \cf2 encoder = OneHotEncoder(sparse=False)
\f0 \cf2 \
				\uc0\u9674  prevents downstream models from making an assumption about ordering\
				\uc0\u9674  creates new columns \
					\'86 as many as there are categories\
					\'86 values are 1 for the intended category and 0 for everything else\
				
\f1 \cf2 sparse=False
\f0 \cf2  \
					\'86 is used just for easier visualization of the data\
					\'86 sparse matrices are efficient data structures when most of the matrix is zero\
			
\f1 \cf2 encoder.get_feature_names(data_categorical.columns)
\f0 \cf2  \
				\uc0\u9674  informative column names can be provided by encoder object\
					\'86 then passed to a dataframe: 
\f1 \cf2 columns=columns_encoded
\f0 \cf2 \
				\uc0\u9674  eg, this made the number of features 10 times largers\
			
\f1 \cf2 data_encoded = encoder.fit_transform(data_categorical)\
				
\f0 \cf2 \uc0\u9674  creates a data set with 102 columns\
			
\f1 \cf2 columns_encoded = encoder.get_feature_names_out\
												(data_categorical.columns)\
			pd.DataFrame(data_encoded, columns=columns_encoded).head()
\f0 \cf2 \
				\uc0\u9674  Wraps NumPy array in a Dataframe\
				\uc0\u9674  Column names generated by the one-hot encoder\
		\'95 Choosing an encoding strategy\
			\'ac Linear models \
				\uc0\u9674  impacted by ordering\
					\'86 use One Hot\
					\'86 or use Ordinal but make sure that:\
						\'bb original categories do have an order\
						\'bb encoded categories follow the same ordering\
			\'ac Tree-based	\
				\uc0\u9674  not impacted by inaccurate ordering\
				\uc0\u9674  use Ordinal\
			\'ac 
\f3\b \cf2 Cardinality
\f0\b0 \cf2 \
				\uc0\u9674  number of unique values in a column\
					\'86 high cardinality causes low computational efficiency for tree-based\
						\'bb creates gigantic, mostly empty matrices\
					\'86 avoid one-hot even if categories are unordered\
		\'95 Evaluate our predictive pipeline\
			\'ac Rare instances in a category\
				\uc0\u9674  attribute categories with very few occurances could be skipped in the train/test split\
					\'86 if that sample ended up only in the test set, classifier would be unable to encode\
				\uc0\u9674  Two solutions\
					\'86 list all possible categories and give to encoder with 
\f1 \cf2 categories
\f0 \cf2  argument\
					\'86 use parameter 
\f1 \cf2 handle_unknown
\f0 \cf2 \
						\'bb 
\f1 \cf2 OrdinalEncoder
\f0 \cf2  also has this to handle rare categories\
			
\f1 \cf2 model = make_pipeline(OneHotEncoder(handle_unknown="ignore"),\
 									LogisticRegression(max_iter=500))
\f0 \cf2 \
				\uc0\u9674  example:\
					
\f1 \cf2 max_iter=500
\f0 \cf2 \
						\'bb otherwise 
\f1 \cf2 ConvergenceWarning
\f0 \cf2 \
					\'86 one-hot does NOT benefit from scaling\
						\'bb all values are on the same scale: 1 or 0\
		\'95 Create categorical logistic regression and check generalization performance:\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1960\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth10900\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth10900\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.pipeline import make_pipeline\
from sklearn.linear_model import LogisticRegression\
\
model = make_pipeline(\
    OneHotEncoder(handle_unknown="ignore"), LogisticRegression(max_iter=500)\
)\
\
from sklearn.model_selection import cross_validate\
cv_results = cross_validate(model, data_categorical, target)\
cv_results
\f0 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 1.3.3 \cf2 \ul \ulc4 Exercise M1.04: Arbitrary Integer Encoding
\f0\b0 \cf2 \ulnone \
	\'95 03_categorical_pipeline_ex_01.ipynb\
	\'95 Performance comparison between OrdinalEncoder and OneHotEncoder (from previous secion)\
	\'95 Load the data\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1120\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth11860\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth11860\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import pandas as pd\
\
adult_census = pd.read_csv("../datasets/adult-census.csv")\
target_name = "class"\
target = adult_census[target_name]\
data = adult_census.drop(columns=[target_name, "education-num"])
\f0 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Filter dataset for categorical features only\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1100\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11780\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth100\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.compose import make_column_selector as selector\
\
categorical_columns_selector = selector(dtype_include=object)\
categorical_columns = categorical_columns_selector(data)\
data_categorical = data[categorical_columns]
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Choose OrdinalEncoder and classifier\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1100\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11820\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth60\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.pipeline import make_pipeline\
from sklearn.preprocessing import OrdinalEncoder\
from sklearn.linear_model import LogisticRegression\
\
model = make_pipeline(OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1), LogisticRegression(max_iter=500))
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Score the accuracy\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1100\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11680\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth220\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import cross_validate\
\
results = cross_validate(model, data_categorical, target, error_score="raise")\
results\
score_array = results["test_score"]\
print(f"\{score_array.mean():.3f\} +/- \{score_array.std():.3f\}")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Choose and score OneHotEncoder with classifier\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1120\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11860\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth180\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.preprocessing import OneHotEncoder\
\
model2 = make_pipeline(OneHotEncoder(handle_unknown="ignore"), LogisticRegression(max_iter=500))\
results2 = cross_validate(model2, data_categorical, target, error_score="raise")\
results2\
score_array2 = results2["test_score"]\
print(f"\{score_array2.mean():.3f\} +/- \{score_array2.std():.3f\}")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 1.3.4 \cf2 \ul \ulc4 Using numerical and categorical variables together
\f0\b0 \cf2 \ulnone \
	\'95 03_categorical_pipeline_column_transformer.ipynb\
	\'95 Selection based on data types\
		
\f1 \cf2 make_column_selector(dtype_exclude=object)\
				
\f0 \cf2 OR:
\f1 \cf2 			(dtype_include=object)
\f0 \cf2 \
			\uc0\u9674  Caution:\
				\'86 object data type could include dates which relate to a quantity of elapsed time\
				\'86 ultimately, data types must be manually inspected\
	\'95 Dispatch columns to a specific processor\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1020\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth11960\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth11960\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.preprocessing import OneHotEncoder, StandardScaler\
from sklearn.compose import ColumnTransformer\
\
categorical_preprocessor = OneHotEncoder(handle_unknown="ignore")\
numerical_preprocessor = StandardScaler()\
preprocessor = ColumnTransformer([\
    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\
    ('standard_scaler', numerical_preprocessor, numerical_columns)])
\f0 \cf2 \
	\uc0\u9674  takes a list of tuples as an argument, each listing:\cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				\'86 a name \
				\'86 the preprocessor\
				\'86 a list of column names to apply it to\
			\uc0\u9674  column transformer then:\
				\'86 splits the columns of the original dataset\
				\'86 transforms each subset (calls either 
\f1 \cf2 fit_transform
\f0 \cf2  or 
\f1 \cf2 transform
\f0 \cf2 )\
				\'86 concatinates the transformed subsets\
			\uc0\u9674  it can be combined with a classifier in a pipeline\
		
\f1 \cf2 model = make_pipeline(\
						preprocessor, LogisticRegression(max_iter=500))
\f0 \cf2 \
			\uc0\u9674  model returned can call methods: \
				
\f1 \cf2 model.fit(data_train, target_train)
\f0 \cf2  \
				
\f1 \cf2 model.predict(data_test)
\f0 \cf2 \
				
\f1 \cf2 model.score(data_test, target_test)
\f0 \cf2 \
	\'95 Evaluation of the model with cross-validation\
		
\f1 \cf2 cv_results = cross_validate(model, data, target, cv=5)\
		scores = cv_results["test_score"]\
		f"\{scores.mean():.3f\} +/- \{scores.std():.3f\}")
\f0 \cf2 \
	\'95 Fitting a more powerful model
\f1 \cf2 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1120\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11860\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth180\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.ensemble import HistGradientBoostingClassifier\
from sklearn.preprocessing import OrdinalEncoder\
\
categorical_preprocessor = OrdinalEncoder(handle_unknown="use_encoded_value",\
                                          unknown_value=-1)\
\
preprocessor = ColumnTransformer([\
    ('categorical', categorical_preprocessor, categorical_columns)],\
    remainder="passthrough")\
\
model = make_pipeline(preprocessor, HistGradientBoostingClassifier())
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  Gradient Boosting Trees\
				\'86 Scaling of numerical features not needed\
				\'86 Ordinal encoding works even with inappropriate ordering\
			\uc0\u9674  effective for datasets with:\
				\'86 large number of samples\
				\'86 limited number of features\
				\'86 mix of categorical and numerical\
			\uc0\u9674  popular with data science practitioners\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a71.3.5 \cf2 \ul \ulc4 Exercise M1.05
\f0\b0 \cf2 \ulnone \
	\'95 03_categorical_pipeline_ex_02.ipynb\
	\'95 Reference pipeline\
		\'ac No numerical scaling \
		\'ac Integer-coded categories\
			\uc0\u9674  The mean cross-validation accuracy is: 0.872 +/- 0.003 with a fitting time of 5.867\
			\uc0\u9674  Trees can handle integer-coded categories as long as they are deep enough\
	\'95 Scaling numerical features\
		\'ac Numerical scaling\
			\uc0\u9674  The mean cross-validation accuracy is: 0.873 +/- 0.003 with a fitting time of 5.448\
	\'95 One-hot encoding of categorical variables\
		\'ac 1 or 0 coded with columns for every category\
			\uc0\u9674  The mean cross-validation accuracy is: 0.873 +/- 0.002 with a fitting time of 12.508\
			\uc0\u9674  Much longer training times due to about 10 times more features\
			\uc0\u9674  However, the implementation of 
\f1 \cf2 HistGradientBoostingClassifier
\f0 \cf2  is incomplete\
\

\f3\b \cf2 \'a7 1.3.6 \cf2 \ul \ulc4 How to define a scikit-learn pipeline and visualize it
\f0\b0 \cf2 \ulnone \
	\'95 video_pipeline.ipynb\
	\'95 Table has some missing data\
		\'ac NaN\
	\'95 But only columns with no missing data are used\
		\'ac some numeric and some categorical\
	
\f1 \cf2 SimpleImputer(strategy='median')\
		
\f0 \cf2  \'ac any missing data will be replaced with the median of that column\
	
\f1 \cf2 OneHotEncoder(handle_unknown='ignore')
\f0 \cf2 \
		\'ac ignore:\
			\uc0\u9674  missing values are assigned zeroes everywhere\
	
\f1 \cf2 ColumnTransformer(transformers=[\
    ('num', numeric_transformer, numeric_features),\
    ('cat', categorical_transformer, categorical_features),])
\f0 \cf2 \
		\'ac applies the transformation\
	\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97MODULE 2: \cf2 \ul \ulc4 SELECTING THE BEST MODEL
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Overview
\f0\b0 \cf2 \ulnone \
	\'95 Underfitting and Overfittitting\
		\'ac\'a0ML models are imperfect\
		\'ac Fundamental trade-off\
			\uc0\u9674  modeling flexibility\
			\uc0\u9674  limited dataset size\
	\'95 Train error vs. Test error\
	\'95 Statistical variance and bias\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97MODULE 2.1: \cf2 \ul \ulc4 OVERFITTING AND UNDERFITTING
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Overfitting and Underfitting
\f0\b0 \cf2 \ulnone \
	\'95 https://www.youtube.com/watch?v=xErJGDwWqys\
\
\

\f3\b \cf2 \'a7 2.1.1 \cf2 \ul \ulc4 Cross-validation framework
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_train_test.ipynb\
	\'95 Predicting median housing values\
		\'ac continuous variable instead of discrete\
		\'ac\'a0regression\
\
	
\f3\b \cf2 \'b6 Training error vs testing error
\f0\b0 \cf2 \
		
\f1 \cf2 from sklearn.tree import DecisionTreeRegressor\
		regressor = DecisionTreeRegressor(random_state=0)
\f0 \cf2 \
			\'ac random_state\
				\uc0\u9674  setting to an integer makes the model deterministic\
		
\f1 \cf2 from sklearn.metrics import mean_absolute_error\
		target_predicted = regressor.predict(data)\
		score = mean_absolute_error(target, target_predicted)
\f0 \cf2 \
			\'ac error score is in the units of the target\
			\'ac Returns an error of 0\
				\uc0\u9674  we made predictions on the training data\
			\'ac methodological problem\
				\uc0\u9674  too optimistic prediction error\
		\'95 
\f3\b \cf2 empirical error
\f0\b0 \cf2 \
			\'ac 
\f3\b \cf2 training error
\f0\b0 \cf2 \
			\'ac eg: we just trained a model which minimizes the training error; it was 0\
				\uc0\u9674  our model had memorized the training set\
			\'ac New addition to our evaluation process\
		\'95 
\f3\b \cf2 generalization error
\f0\b0 \cf2 \
			\'ac 
\f3\b \cf2 testing error
\f0\b0 \cf2 \
			\'ac error that comes from predictions on test data not seen during training\
			\'ac score from 
\f1 \cf2 mean_absolute_error
\f0 \cf2  is in the units of target\
	\
	
\f3\b \cf2 \'b6 Stability of the cross-validation estimates
\f0\b0 \cf2 \
		\'95 Small test sets\
			\'ac testing error estimate is unstable\
			\'ac won't reflect "true error rate" of the same model with lots of data\
		\'95 Cross-validation\
			\'ac estimate of the variability in the generalization performance\
		\'95 
\f3\b \cf2 Shuffle-split
\f0\b0 \cf2 \
			\'ac\'a0Randomly shuffle the order\
			\'ac Split the shuffled set\
			\'ac Train\
			\'ac Evaluate testing error with the test set\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-03-30 at 7.19.15 PM.png \width7620 \height2440 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		
\f1 \cf2 from sklearn.model_selection import cross_validate\
		from sklearn.model_selection import ShuffleSplit\
\
		cv = ShuffleSplit(n_splits=40, test_size=0.3, random_state=0)\
		cv_results = cross_validate(\
    	regressor, data, target, cv=cv, scoring="neg_mean_absolute_error")
\f0 \cf2 \
			\'ac Returned as a dictionary\
			
\f1 \cf2 cv
\f0 \cf2 \
				\uc0\u9674  where the ShuffleSplit object is added to a cross-validation\
			
\f1 \cf2 n_split
\f0 \cf2 \
				\uc0\u9674 \'a0computation cost rises with the number of splits\
			
\f1 \cf2 scoring="neg_mean_absolute_error"
\f0 \cf2 \
				\uc0\u9674  The parameter 
\f1 \cf2 score
\f0 \cf2  always expects a function that is a score\
					\'86 Score		Higher values are better results\
					\'86 Error		Lower values are better results				\uc0\u9674  All error metrics can be transformed into a score by making them negative\
			
\f1 \cf2 cv_results["test_error"] = -cv_results["test_score"]
\f0 \cf2 \
				\uc0\u9674  change negative-valued score results to positive-value error\
		\'95 Dictionary -> Dataframe\
			
\f1 \cf2 pd.DataFrame(cv_results)
\f0 \cf2 \
			\'ac makes a nice display\
		
\f1 \cf2 f"\{cv_results['test_error'].mean():.2f\} k$")\
		f"\{cv_results['test_error'].std():.2f\} k$")
\f0 \cf2 \
			\'ac The mean testing error and standard deviation\
			\'ac eg:  46.36 +/- 1.17 k$\
		
\f1 \cf2 f"\{target.std():.2f\} k$")
\f0 \cf2 \
			\'ac Now find the standard deviation of the target column\
			\'ac Note that the error is smaller than this SD\
				\uc0\u9674  So, smaller than the natural scale of variation in the target variable\
			\'ac And the SD of the testing error estimate is even smaller\
		\'95 However more is needed to determine if generalization performance is good enough\
			\'ac Observation of the range of target values show a lower end of 50 k$\
			\'ac The mean testing error is far too large compared to that target value\
			\'ac mean absolute percentage error might be better\
	\
	
\f3\b \cf2 \'b6 More detail regarding cross_validate
\f0\b0 \cf2 \
		
\f1 \cf2 cv_results = cross_validate(regressor, data, target,\
		return_estimator=True)
\f0 \cf2 \
			\'ac will retrieve all fitted models produced by cross-validate\
			\'ac returns them as entries in the dictionary\
			\'ac this allows inspection of the internal fitted parameters\
		
\f1 \cf2 scores = cross_val_score(regressor, data, target)
\f0 \cf2 \
			\'ac just returns the 
\f1 \cf2 test_score
\f0 \cf2  list entry in the cross-validate dictionary\
\
			\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 2.1.2 \cf2 \ul \ulc4 Overfit-generalization-underfit
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_validation_curve.ipynb\
	\'95 Using training and testing errors to determine overfit/generalizing/underfit\
	\
	
\f3\b \cf2 \'b6 Overfitting vs. underfitting
\f0\b0 \cf2 \
		\'95\'a0Comparison of testing and training error\
		\'95 Using 
\f1 \cf2 cross_validate
\f0 \cf2  with:\
			
\f1 \cf2 neg_mean_absolute_error\
			ShuffleSplit\
			return_train_score=True
\f0 \cf2 \
				\uc0\u9674  returns training score as well as test score\
		\'95 Change negative error into positive\
			
\f1 \cf2 scores[["train error", "test error"]] = \
								-cv_results[["train_score", "test_score"]]
\f0 \cf2 \
		\'95\'a0Plot both training and testing error\
			\'ac\'a0Very small training error\
				\uc0\u9674  basically 0\
				\uc0\u9674  Not underfitting\
					\'86 flexible enough to capture training set variability\
			\'ac Larger testing error\
				\uc0\u9674  on the same order of magnitude as the smallest values in the dataset\
				\uc0\u9674  the model memorized the many variations in the training set\
					\'86 noisy\
					\'86 Do not generalize well\
				\uc0\u9674  Is overfitting\
		\
	
\f3\b \cf2 \'b6 Validation curve\
		
\f1\b0 \cf2 from sklearn.model_selection import validation_curve\
		from sklearn.model_selection import ShuffleSplit\
\
		cv = ShuffleSplit(n_splits=30, test_size=0.2)\
\
		max_depth = [1, 5, 10, 15, 20, 25]\
		train_scores, test_scores = validation_curve(\
    				regressor, data, target, param_name="max_depth", \
					param_range=max_depth, cv=cv, 					scoring="neg_mean_absolute_error", n_jobs=2)\
		train_errors, test_errors = -train_scores, -test_scores
\f0 \cf2 \
			\'ac Model hyperparameters can be adjusted to go from underfitting to overfitting\
			\'ac A validation curve plots training and testing error over various maximum depths\
			
\f1 \cf2 param_range
\f0 \cf2 \
				\uc0\u9674  array of numbers to be used as parameters for each individual model\
			'
\f1 \cf2 gamma'
\f0 \cf2 \
				\uc0\u9674  array of numbers to be used as parameters\
			'
\f1 \cf2 svc__gamma'
\f0 \cf2 \
				\uc0\u9674  required when using a pipeline instead of just the regressor\
			\'ac Use 
\f1 \cf2 model.get_params().keys()
\f0 \cf2  to figure out parameters available
\fs24 \cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4 \cf2 \
		\cf2 {{\NeXTGraphic Screen Shot 2022-03-30 at 5.40.34 PM.png \width8120 \height6120 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		\
		\'95 Curve has three areas:\
			<10		Underfitting	Both training and testing error are high\
								Model is too constrained, cannot capture variability\
			=10		Generalizing	Flexible enough to capture variability\
								Avoids memorizing the "noise" in the target data\
			<10		Overfitting		Training error shrinks but testing error increases\
								Creates decisions for noisy samples harming generalization\
		\'95 At =10\
			\'ac A little overfitting, shown as the gap between errors\
			\'ac\'a0Also shows underfitting, since error is still far from zero\
			\'ac However, best compromise by tuning just this parameter\
		
\f1 \cf2 plt.errorbar(max_depth, train_errors.mean(axis=1),\
             yerr=train_errors.std(axis=1), label='Training error')\
		plt.errorbar(max_depth, test_errors.mean(axis=1),\
             yerr=test_errors.std(axis=1), label='Testing error')
\f0 \cf2 \
				\uc0\u9674  Standard deviation\
					\'86 Just looking at mean errors (above) is limited\
					\'86 Add SD bars to more accurately graph the analysis\
		\'95 Let's go back and find the tree depth of the original model\
			\'ac Now that we know tree depth determines error levels\
			\'ac original 
\f1 \cf2 DecisionTreeRegressor
\f0 \cf2  showed a mean testing error of 47.28\
				\uc0\u9674  it also showed 0 for training error (see below)\
			\'ac The range of errors in the graph showed that error level was near a depth of 8... maybe.\
			\'ac The array of testing errors also shows the same\
				
\f1 \cf2 test_errors.mean(axis=1)
\f0 \cf2 \
			\'ac Looking at docs, 
\f1 \cf2 DecisionTreeRegressor
\f0 \cf2  exposes a method to return depth:\
				
\f1 \cf2 regressor.get_depth()
\f0 \cf2 \
				\uc0\u9674  requires regressor be fitted with 
\f1 \cf2 .fit
\f0 \cf2  first\
			\'ac Gives a number of 36... which means we're way off the right side of the graph.\
				\uc0\u9674  and makes sense if training error is 0...\
					
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-03-30 at 6.38.37 PM.png \width9720 \height6100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\cf2 				
\f0\fs30 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 2.1.3 \cf2 \ul \ulc4 Effect of the sample size in cross-validation
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_learning_curve.ipynb\
	\
	
\f3\b \cf2 \'b6 Learning curve
\f0\b0 \cf2 \
		\'95 training and testing scores plotted against various training set sizes\
		
\f1 \cf2 results = learning_curve(\
		    regressor, data, target, train_sizes=train_sizes, cv=cv,\
    		scoring="neg_mean_absolute_error", n_jobs=2)
\f0 \cf2 \
				\uc0\u9674  argument 
\f1 \cf2 train_sizes
\f0 \cf2  varies through a list of fractions between .1 and 1\
		
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-03-30 at 7.08.49 PM.png \width8180 \height6140 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
			\'ac Training error shows overfitting\
			\'ac Testing error decreases as more samples are added\
				\uc0\u9674  there should eventually be a plateau\
				\uc0\u9674  which would show limits of the available model:\
					\'86 
\f3\b \cf2 Bayes error rate
\f0\b0 \cf2 \
	\

\f3\b \cf2 \'a7 2.1.4 \cf2 \ul \ulc4 Excercise M2.01
\f0\b0 \cf2 \ulnone \
	\'95 Retrieve parameter names from a model\
		
\f1 \cf2 model.get_params().keys()\
	
\f0 \cf2 \'95 Regular intervals including the endpoints\
		
\f1 \cf2 np.linspace(0.1, 1.0, num=5, endpoint=True)
\f0 \cf2 \
	\'95 Logarithmic scale regular intervals\
		
\f1 \cf2 np.logspace(-3, 2, num=30)
\f0 \cf2 \
	\'95 Line transparency and little error bars\
		
\f1 \cf2 plt.errorbar(x, y, yerr=d, alpha=0.5)
\f0 \cf2 \
	\'95 Logarithmic graphing\
		
\f1 \cf2 matplotlib.pyplot.xscale("log")
\f0 \cf2 \
	\'95 Learning Curve\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-03-31 at 10.51.25 PM.png \width8620 \height5700 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
			\'ac Adding new samples does not really improve accuracy for either line\
			\'ac Testing oscillates around 76%\
				\uc0\u9674  Turns out 76% of the samples belong to "not donated"\
			\'ac DummyClassifier set to "not donated" would achieve an accuracy of 76%\
				\uc0\u9674  Maybe the small pipeline is not able to use input features to improve\
				\uc0\u9674  Maybe the input features are not very informative\
				\uc0\u9674  Maybe the hyperparameter value of the SVC was poor\
				\uc0\u9674  Maybe the choice of SVC wasn't the best option\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97MODULE 2.3: \cf2 \ul \ulc4 BIAS VS VARIANCE TRADE-OFF
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 2.3.1 \cf2 \ul \ulc4 Bias vs Variance
\f0\b0 \cf2 \ulnone \
	\'95 https://www.youtube.com/watch?v=VOeTTiML1pY&feature=youtu.be\
	\'95 Resampling the training set\
		\'ac Limited amount of training data\
		\'ac Training set is a small, random subset of all possible\
		\'ac How does choosing the set effect the learned prediction outcome?\
	\'95 Overfit\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.32.46 PM.png \width3740 \height5220 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.31.34 PM.png \width3820 \height5180 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.31.44 PM.png \width3860 \height4840 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\'ac Degree nine function modeling\
				\uc0\u9674  But if we access to a different training set\
				\uc0\u9674  We'd get a different model\
					\'86\'a0and different predictions\
				\uc0\u9674  but prediction error would be about the same\
			\'ac Exessive overfitting\
				\uc0\u9674  yellow patch\
				\uc0\u9674  the weird shape around those three points captures noise\
					\'86\'a0but isn't representative of the overall trends\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.34.43 PM.png \width4280 \height5200 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		\
				\'ac The average model might be pretty good\
					\uc0\u9674  but each individual is overfitting and is bad\
	\'95 Underfit\
		
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.35.39 PM.png \width4120 \height5160 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.36.53 PM.png \width4160 \height5220 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\'ac if we change the sample of the training set\
				\uc0\u9674  the model does not change much\
				\uc0\u9674  slope is basically the same\
		
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.37.37 PM.png \width4080 \height5120 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\'ac They make the same kind of errors\
				\uc0\u9674  they all make a systematic variance\
					\'86 in the yellow patch, they all underestimate\
					\'86 in the center, they all overestimate\
				\uc0\u9674  bias\
	\'95 Underfit vs Overfit\

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-04-02 at 7.38.20 PM.png \width7680 \height5260 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\'ac Bias/Underfit\
				\uc0\u9674  
\f3\b \cf2 mispecified models
\f0\b0 \cf2 \
				\uc0\u9674  All models make a systematic prediction error\
				\uc0\u9674  model prefers to ignore some aspects of the data\
				\uc0\u9674  On average they are not good\
			\'ac Variance/Overfit\
				\uc0\u9674  
\f3\b \cf2 unstable models
\f0\b0 \cf2 \
					\'86 too much flexibility\
				\uc0\u9674  On average, they can be good\
					\'86 but prediction errors are without obvious structure\
				\uc0\u9674  Have high sensitivity to the training set used\
				\uc0\u9674  also have bad individual test errors\
	\'95 Bias-variance decomposition of the Mean Squared Error\
		\'ac see Wikipedia\
		\'ac the average of the squares of the errors\
			\uc0\u9674  variance\
				\'86 how widely spread the estimates are from one data sample to another\
			\uc0\u9674  bias\
				\'86 how far off the average estimated value is from the true value\
		\'ac root mean square deviation\
			\uc0\u9674  square root of the MSE\
			\uc0\u9674  same units as the quantity being estimated\
			\uc0\u9674  for unbiased estimators, it is the standard deviationn\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 2.4 \cf2 \ul \ulc4 Quiz
\f0\b0 \cf2 \ulnone \
	\'95 Show all possible values in a column\
		
\f1 \cf2 target.value_counts()
\f0 \cf2 \
		
\f1 \cf2 target.unique()
\f0 \cf2 \
	\'95 Returns just the values for the 
\f1 \cf2 'test_score'
\f0 \cf2  key from the cross_validate dictionary\
		
\f1 \cf2 cross_val_score(model, X, y)\
	
\f0 \cf2 \'95 Returns a more accurate assessment of a dummy_classifier from cross_validate/cross_val_score\
		
\f1 \cf2 scoring="balanced_accuracy"
\f0 \cf2 \
	\'95\'a0Scaling required for KNeighborsClassifier\
		\'ac Classifier compares the y-values of the K-closest neighbors to the test data\
		\'ac\'a0Model computes distances between data points\
			price:			0-10000000\
			temperature:	0-30\
		\'ac Distances between rows will be mostly impacted by price\
			\uc0\u9674  temperature will be ignored\
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Wrap-up
\f0\b0 \cf2 \ulnone \
	\'95 Overfitting caused by:\
		\'ac limited size of training set\
		\'ac noise in data\
		\'ac high flexibility of ML model\
	\'95 Underfitting / systematic errors caused by:\
		\'ac model and parameter choice\
			\uc0\u9674  lack of flexibility can't capture structure of the true data\
	\'95 Fixed training set\
		\'ac minimize test error by adjusting model and parameters\
	\'95 Fixed model and parameters\
		\'ac increasing training set size can:\
			\uc0\u9674  decrease overfitting\
			\uc0\u9674  increase underfitting\
	\'95 Model which is neither overfitting nor underfitting\
		\'ac high error remains if:\
			\uc0\u9674  variations in the target cannot be largely determined by the features\
		\'ac 
\f3\b \cf2 label noise
\f0\b0 \cf2 \
			\uc0\u9674  irreducible error\
			\uc0\u9674  critical feature data missing	\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97\cf2 \ul \ulc4 MODULE 3: HYPERPARAMETER TUNING
\f0\b0 \cf2 \ulnone \

\f3\b \cf2 \
\'a7 \cf2 \ul \ulc4 Overview
\f0\b0 \cf2 \ulc4 \
	\cf2 \ulnone \'95 
\f3\b \cf2 hyperparameters
\f0\b0 \cf2 \
		\'ac control the learning process\
		\'ac often manually tuned\
		\'ac cannot be estimated from the data\
	\'95 Do not confuse hyperparameters with parameters inferred during the training\
		\'ac\'a0parameters define the model itself\
		\'ac\'a0eg, coefficients in a linear model, 
\f1 \cf2 model.coef_
\f0 \cf2 \
	\'95\'a0Objectives\
		\'ac How to get and set hyperparameter values\
		\'ac Fine tune full predictive modeling pipeline\
		\'ac understand and visualize improved parameter combinations
\f3\b \cf2 \
\
\'a7 3.1.1 \cf2 \ul \ulc4 Set and get hyperparameters in scikit-learn
\f0\b0 \cf2 \ulnone \
	\'95 parameter_tuning_manual.ipynb\
	\'95 New pipeline function\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 model = Pipeline(steps=[\
    ("preprocessor", StandardScaler()),\
    ("classifier", LogisticRegression())\
])
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Setting parameter C when creating the model\
		
\f1 \cf2 LogisticRegression(C=1e-3)
\f0 \cf2 \
	\'95 Setting parameter C after model is generated\
		
\f1 \cf2 model.set_params(classifier__C=1e-3)
\f0 \cf2 \
	\'95 Pipeline parameter names:\
		 
\f1 \cf2 <model_name>__<parameter_name>
\f0 \cf2 \
	\'95 List all parameter names and values\
		
\f1 \cf2 model.get_params()\
			
\f0 \cf2 \uc0\u9674  Get only parameter names\

\f1 \cf2 				for parameter in model.get_params():\
   					print(parameter)
\f0 \cf2 \
			\uc0\u9674  Get only the value of a parameter\
				
\f1 \cf2 model.get_params()['classifier__C']
\f0 \cf2 \
	\'95 Varying parameter C manually\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 for C in [1e-3, 1e-2, 1e-1, 1, 10]:\
    model.set_params(classifier__C=C)\
    cv_results = cross_validate(model, data, target)\
    scores = cv_results["test_score"]\
    print(f"Accuracy score via cross-validation with C=\{C\}:\\n"\
          f"\{scores.mean():.3f\} +/- \{scores.std():.3f\}")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Warning\
		\'ac When we evaluate a family of models on test data and pick the best:\
			\uc0\u9674  Prediction accuracy cannot be trusted\
			\uc0\u9674  Test data has been used to select the model and is no longer independent\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 3.1.2 \cf2 \ul \ulc4 Excercise M3.01
\f0\b0 \cf2 \ulnone \
	\'95 parameter_tuning_ex_02.ipynb\
	\'95 \
\

\f3\b \cf2 \'a7 3.2.1 \cf2 \ul \ulc4 Hyperparameter tuning by grid-search
\f0\b0 \cf2 \ulnone \
	\'95 parameter_tuning_grid_search.ipynb\
	\'95 Categorical features\
		\'ac\'a0Select categorical columns\
			
\f1 \cf2 categorical_columns_selector = \
								make_column_selector(dtype_include=object)\
			categorical_columns = categorical_columns_selector(data)
\f0 \cf2 \
	\'95\'a0Usinng a tree-based model means:\
		\'ac Numerical variables don't need scaling\
		\'ac Categorical variables can be dealt with an OrdinalEncoder\
			\uc0\u9674  even though coding order has no meaning\
			\uc0\u9674  OrdinalEncoder avoids high-dimensional representations (ie: OneHot)\
	\'95\'a0Unknown values for OrdinalEncoder should be set to -1\
		
\f1 \cf2 categorical_preprocessor = OrdinalEncoder(\
				handle_unknown="use_encoded_value", unknown_value=-1)
\f0 \cf2 \
	\'95 GridSearchCV accomplishes search similar to the two 
\f1 \cf2 for
\f0 \cf2  loops\
		
\f1 \cf2 from sklearn.model_selection import GridSearchCV
\f0 \cf2 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 param_grid = \{\
    'classifier__learning_rate': (0.01, 0.1, 1, 10),\
    'classifier__max_leaf_nodes': (3, 10, 30)\}\
model_grid_search = GridSearchCV(model, param_grid=param_grid,\
                                 n_jobs=2, cv=2)\
model_grid_search.fit(data_train, target_train)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\'ac Returns the best model\
				\uc0\u9674  which still needs to be fitted\
			\'ac Pass model along with test data to cross_validate to score\
			\'ac\'a0
\f1 \cf2 param_grid
\f0 \cf2  needs to be a dictionary\
				\uc0\u9674  key names in grid dict must come from 
\f1 \cf2 model.get_params().keys()
\f0 \cf2 \
			\'ac 
\f1 \cf2 n_jobs
\f0 \cf2  sets the number of cores for simultaneous processing\
				\uc0\u9674  
\f1 \cf2 -1
\f0 \cf2  will use all available cores\
			\'ac 
\f1 \cf2 gridsearch.cv_results_
\f0 \cf2 \
				\uc0\u9674  shows all grid search data\
	\'95 Model from grid-search has attribute which displays the chosen parameters\
		
\f1 \cf2 model_grid_search.best_params_
\f0 \cf2 \
	\'95 Score results were similar to the for loop method\
		\'ac but it still came up with a different answer from the grid\
	\'95 All scoring results from all permutations are stored in an attribute\
		
\f1 \cf2 model_grid_search.cv_results_
\f0 \cf2 \
	\'95 Heatmap\
		\'ac\'a0Create a dataframe of all scores\
			
\f1 \cf2 cv_results = pd.DataFrame(\
					model_grid_search.cv_results_).sort_values(\
    				"mean_test_score", ascending=False)
\f0 \cf2 \
		\'ac Column name management\
			\uc0\u9674  Add "
\f1 \cf2 param_
\f0 \cf2 " to the beginning of each of the 
\f1 \cf2 param_grid
\f0 \cf2  keys\
				
\f1 \cf2 column_results = \
					[f"param_\{name\}" for name in param_grid.keys()]
\f0 \cf2 \
			\uc0\u9674  then include names for columns of mean score, SD score, and rank\
				
\f1 \cf2 column_results += \
					["mean_test_score", "std_test_score","rank_test_score"]
\f0 \cf2 \
			\uc0\u9674  Use those to make a new dataframe\
				
\f1 \cf2 cv_results = cv_results[column_results]
\f0 \cf2 \
			\uc0\u9674  Drop the 
\f1 \cf2 param_classifier__
\f0 \cf2  from the name\
				
\f1 \cf2 if "__" in param_name:\
        			return param_name.rsplit("__", 1)[1]
\f0 \cf2 \
			\uc0\u9674  Transform two parameters and mean test scores into pivot table\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2760\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10080\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 pivoted_cv_results = cv_results.pivot_table(\
    values="mean_test_score", index=["learning_rate"],\
    columns=["max_leaf_nodes"])
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  Graph a heatmap\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2940\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth9900\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth9900\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import seaborn as sns\
\
ax = sns.heatmap(pivoted_cv_results, annot=True, cmap="YlGnBu", vmin=0.7,vmax=0.9)\
ax.invert_yaxis(s)
\f0 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95\'a0Visual interpretation\
		
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-04 at 10.13.06 PM.png \width8220 \height5800 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
			\'ac\'a0For very High values of 
\f1 \cf2 learning_rate:
\f0 \cf2 		\
				\uc0\u9674  Adjusting 
\f1 \cf2 max_leaf_nodes
\f0 \cf2  cannot help\
			\'ac\'a0Otherwise:\
				\uc0\u9674  As 
\f1 \cf2 max_leaf_node
\f0 \cf2  increases, 
\f1 \cf2 learning_rate
\f0 \cf2  must decrease\
	\'95 No unique optimal parameter setting\
		\'ac 4 models out of 12 configurations reach maximal accuracy\
			\uc0\u9674  \'b1 random fluxuations caused by sampling\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 3.2.2 \cf2 \ul \ulc4 Hyperparameter tuning by randomized-search
\f0\b0 \cf2 \ulnone \
	\'95 parameter_tuning_randomized_search.ipynb\
	\'95 Grid-search shortcomings\
		\'ac Does not scale when number of parameters to tune is increasing \
			\uc0\u9674  they must be specified explicitly, which becomes unworkable in very large numbers\
		\'ac Imposes search regularity (vs. randomness) via intervals which can be problematic\
			\uc0\u9674  ideal values might lie between the intervals\
	
\f3\b \cf2 \'b6 Our predictive model
\f0\b0 \cf2 \
		\'95 Random parameter candidates on top of grid search candidates:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-04-05 at 1.21.35 PM.png \width11260 \height9960 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\'ac adding more evaluations oncreases resolution in each directions\
			\'ac because hyperparameter 2 is unimportant, region of good parameters aligns with grid\
			\'ac 
\f3\b \cf2 stochastic search
\f0\b0 \cf2 \
				\uc0\u9674  Use of randomness in the optimization algorithm\
		\'95 RandomizedSearchCV works like GridSearchCV\
			\'ac But sampling distributions instead of parameter values\
			\'ac\'a0eg: log-uniform distribution\
				\uc0\u9674  the parameters of interest have positive values and natural log scaling\
				\uc0\u9674  0.1 is as close to 1 as 10 is\
		\'95 Three new parameters are specified in addition to learning_rate and max_leaf_nodes:\
			
\f1 \cf2 l2_regularization			
\f0 \cf2 \'ac strength of regularization\
			
\f1 \cf2 min_samples_leaf			
\f0 \cf2 \'ac min number of samples req'd in a leaf\
			
\f1 \cf2 max_bins					
\f0 \cf2 \'ac max number of bins for histogram\
			
\f1 \cf2 learning_rate
\f0 \cf2 				\'ac speed that gradient-boosting corrects residuals\
			
\f1 \cf2 max_leaf_nodes
\f0 \cf2 				\'ac max number of leaves for each tree\
		\'95 Object that produces logarithmically-even distribution of random integers\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1920\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10920\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from scipy.stats import loguniform\
\
class loguniform_int:\
    """Integer valued version of the log-uniform distribution"""\
    def __init__(self, a, b):\
        self._distribution = loguniform(a, b)\
\
    def rvs(self, *args, **kwargs):\
        """Random variable sample"""\
        return self._distribution.rvs(*args, **kwargs).astype(int)\
\
loguniform_int.rvs(0, 100)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 RandomSearchCV implemented\
				\uc0\u9674  Bottom three hyperparameters need integers\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2020\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth120\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 %%time\
from sklearn.model_selection import RandomizedSearchCV\
\
param_distributions = \{\
    'classifier__l2_regularization': loguniform(1e-6, 1e3),\
    'classifier__learning_rate': loguniform(0.001, 10),\
    'classifier__max_leaf_nodes': loguniform_int(2, 256),\
    'classifier__min_samples_leaf': loguniform_int(1, 100),\
    'classifier__max_bins': loguniform_int(2, 255),\
\}\
\
model_random_search = RandomizedSearchCV(\
    model, param_distributions=param_distributions, n_iter=10,\
    cv=5, verbose=1,\
)\
model_random_search.fit(data_train, target_train)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 Then compute the accuracy with the test set\
			
\f1 \cf2 accuracy = model_random_search.score(data_test, target_test)
\f0 \cf2 \
		\'95 Display results with modified formatting\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2020\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 def shorten_param(param_name):\
    if "__" in param_name:\
        return param_name.rsplit("__", 1)[1]\
    return param_name\
\
# get the parameter names\
column_results = [\
    f"param_\{name\}" for name in param_distributions.keys()]\
column_results += [\
    "mean_test_score", "std_test_score", "rank_test_score"]\
\
cv_results = pd.DataFrame(model_random_search.cv_results_)\
cv_results = cv_results[column_results].sort_values(\
    "mean_test_score", ascending=False)\
cv_results = cv_results.rename(shorten_param, axis=1)\
cv_results
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 Evaluation\
			\'ac Best model is substantially better then the second to best\
				\uc0\u9674  the difference between the two mean test scores is almost 3 times the SD of the best\
			
\f1 \cf2 cv_results = cv_results.set_index("rank_test_score")\
			cv_results["mean_test_score"][1] - \
										cv_results["mean_test_score"][2]
\f0 \cf2 \
			
\f1 \cf2 3 * cv_results["std_test_score"][1]
\f0 \cf2 \
		\'95 Remember that better parameter sets could be possible but weren't tested in the search\
			\'ac\'a0a search with
\f1 \cf2  n_iter=200
\f0 \cf2  can produce even more parameter results\
			\'ac this data shows there is high overlap of mean \'b1 SD between top performing models\
				\uc0\u9674  Therefore, no unique best set of parameters\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 3.2.3 \cf2 \ul \ulc4 Analysis of hyperparemeter search results
\f0\b0 \cf2 \ulnone \
	\'95\'a0Scatterplot / heatmap\
		\'ac Sometimes can see bands in the data\
	\'95 Parallel Coordinate Plot\
		\'ac\'a0Tool can show what ranges of parameters produce certain results\
		
\f1 \cf2 import plotly.express as px\
		fig = px.parallel_coordinates(cv_results.apply(\{...\}), color=...\
		fig.show
\f0 \cf2 \
\

\f3\b \cf2 \'a7 3.2.4 \cf2 \ul \ulc4 Analysis of hyperparameter search results
\f0\b0 \cf2 \ulnone 	\
	\'95\'a0Heatmap\
		\'ac only works with two parameters for visualization\
		\'ac can be done pair-wise, but this leads to wrong interpretation of the data\
	\'95 Parallel Coordinate Plot\
		\'ac axis values transformed with log
\fs24 \cf2 \sub 10
\fs30 \cf2 \nosupersub  and log
\fs24 \cf2 \sub 2
\fs30 \cf2 \nosupersub  \
		\'ac improves readability\
		\'ac don't forget to transform back to understand the actual parameter values indicated\
	\'95 Example\
		\'ac Selecting learning_rate = -1.5 to -0.5 and max_bins = 5 to 8			\uc0\u9674  all other parameters will still select the top performing models\
				\'86 other parameters are not very sensitive\
\

\f3\b \cf2 \'a7 3.2.5 \cf2 \ul \ulc4 Evaluation and hyperparameter tuning
\f0\b0 \cf2 \ulnone \
	\'95 parameter_tuning_nested.ipynb\
	\'95 Always return dense matrices from ColumnTransformer\
		
\f1 \cf2 ColumnTransformer(sparse_threshold=0,...)
\f0 \cf2 \
	\'95 Best parameters from a GridSearchCV\
		
\f1 \cf2 model_grid_search.best_params_
\f0 \cf2 \
	\'95 Evaluation of generalization performance\
		\'ac\'a0Mean/SD of grid-search/cross-validation scores might not be good generalization estimates\
		\'ac When refitting a model using the best hyperparameters on the full dataset\
			
\f1 \cf2 model_grid_search.fit
\f0 \cf2 \
				\'86 method automatically performs this refit by default\
			\uc0\u9674  Knowledge from the full dataset is therefore used to both: \
				\'86 decide hyperparameters \
				\'86 train model\
		\'ac Best practice is then to keep an external test set held out for final evaluation of refitted model\
			\uc0\u9674  Basically:\
				\'86 Use full data for GridSearchCV\
					
\f1 \cf2 model_grid_search = GridSearchCV(\
    						model, param_grid=param_grid, n_jobs=2, cv=2
\f0 \cf2 \
						\'87 Which doesn't actually calculate anything\
				\'86 Split and use train_data for fitting\
					
\f1 \cf2 model_grid_search.fit(data_train, target_train)
\f0 \cf2 \
						\'87 Actually performs the grid-search\
				\'86 Use test_data for scoring
\f1 \cf2 \
					accuracy = model_grid_search.score(\
						data_test, target_test)
\f0 \cf2 \
	\'95 Score analysis\
		\'ac Final test set score is almost in the range of the best internal CV score\
			\uc0\u9674  This means tuning procedure did not cause overfitting\
				\'86 Otherwise final score would have significantly lower than internal CV score\
				\'86 No overfitting is expected, however, since few hyperparameters used in grid-search\
		\'ac Final score is higher than expected with internal CV loop/grid-search\
			\uc0\u9674  Expected because the final fit is done on the entire red/green set\
				\'86 Each CV iteration is done on a single green/red line\
				\'86 Models tuned on larger number of samples usually generalize better\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-09 at 1.21.45 PM.png \width10420 \height2700 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\uc0\u9674  K-fold\
				\'86 each line trains on red, evaluates with green\
					\'bb best hyperparameters are chosen from these intermediate evaluation scores\
				\'86 final model with those hyperparameters is fitted using entire red/green set\
					\'bb evaluated with blue samples\
		\'ac 
\f3\b \cf2 Nested cross-validation
\f0\b0 \cf2 \
			\uc0\u9674  Provides a range of uncertainly to complement the single-point generalization estimate\
			\uc0\u9674  Instead of using 
\f1 \cf2 .fit
\f0 \cf2  and 
\f1 \cf2 .score
\f0 \cf2  on the GridSearchCV model\
			\uc0\u9674  Use GridSearchCV model as argument for cross_validate\
				
\f1 \cf2 cv_results = cross_validate(\
    				model_grid_search, data, target, cv=5, n_jobs=2, \
					return_estimator=True)
\f0 \cf2 \
						\'87 Inner cross-validation for the hyperparameter selection\
						\'87 Outer cross-validation for generalization performance evaluation\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-09 at 3.31.20 PM.png \width11100 \height11100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
				
\f1 \cf2 cv_inner = KFold(n_splits=4)
\f0 \cf2 \
					\'bb indexed on the left side\
					\'bb each split trains a model on the red samples\
					\'bb then evaluates the hyperparameter quality with the green samples
\f1 \cf2 \
				cv_outer = KFold(n_splits=5)
\f0 \cf2 \
					\'bb indexed on right side\
					\'bb best hyperparameters are chosen from their validation scores using green samples\
					\'bb model is refitted using combined red/green samples of that outer CV iteration\
				\'86 Generalization performance of 5 models from outer CV loop are evaluated with blue\
		\'ac Comparison of the best hyperparameters\
			\uc0\u9674  Check values of the best hyperparameters\
				
\f1 \cf2 return_estimator=True
\f0 \cf2 \
					\'bb must be used as an argument in the cross_validate\
				
\f1 \cf2 estimator_in_fold.best_params_
\f0 \cf2 \
			\uc0\u9674  If all values are similar, it means we can expect performance close to measured\
			\uc0\u9674  If different tuning sessions give different results, then any value will work\
				\'86 Try a parallel coordinate plot to observe if certain hyperparameters don't matter\
			\uc0\u9674  Or, just use all the outputted models and give each a vote\
				\'86 but very computationally expensive\
\

\f3\b \cf2 \'a7 3.2.6 \cf2 \ul \ulc4 Exercise M3.02 and Quiz 3
\f0\b0 \cf2 \ulnone \
	\'95 parameter_tuning_ex_03.ipynb\
	\'95 Creates dataframe with only certain columns and dropping data samples with missing values\
		
\f1 \cf2 penguins_non_missing = penguins[columns + [target_name]].dropna()
\f0 \cf2 \
	\'95 Box-Cox method\
		\'ac Common preprocessing strategy for positive values\
			
\f1 \cf2 PowerTransformer(method="box-cox")
\f0 \cf2 \
	\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97 \cf2 \ul \ulc4 MODULE 4.1: LINEAR REGRESSION
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7  \cf2 \ul \ulc4 Intuitions on linear models
\f0\b0 \cf2 \ulnone \
	\'95 https://www.youtube.com/watch?v=ksEGivkPP7I\
	\'95\'a0Find a relationship between output variable and input variables\
		\'ac linear combination of those input variables\
		\'ac approximate output by taking the weighted sum of the input variable\
			\uc0\u9674  eg, feature data * coefficient + feature data * coefficient + ... + intercept\
	\'95\'a0Machine Learning\
		\'ac these coefficients are learned (or tuned) from the training set\
\

\f3\b \cf2 \'a7 4.1.1 \cf2 \ul \ulc4 Linear regression without scikit-learn
\f0\b0 \cf2 \ulnone \
	\'95 linear_regression_without_sklearn.ipynb\
	\'95 Using penguin dataset with flipper length and body mass\
	\'95 The object returned by calling scatterplot can be used to add additional labels\
		
\f1 \cf2 ax = sns.scatterplot(data=penguins, x=feature_name, y=target_name,\
                     		color="black", alpha=0.5)\
		ax.set_title("Flipper length in function of the body mass")
\f0 \cf2 	\'95 Regression problem\
		\'ac\'a0Penguin mass\
		\'ac Continuous variable\
		\'ac\'a0Range: 2700g and 6300g\
	\'95 Linear relationship\
		
\f1 \cf2 body_mass = weight_flipper_length * flipper_length + \
					intercept_body_mass
\f0 \cf2 \
		\'ac variable 
\f1 \cf2 weight_flipper_length
\f0 \cf2  is a weight (parameter) applied to flipper length\
			\uc0\u9674  positive weight: body mass goes up with flipper length\
		\'ac the coefficient (weight/parameter)  has units of g/mm\
		\'ac y-intercept hits 1500g at 0mm flipper length\
	\

\f3\b \cf2 \'a7 4.1.2 \cf2 \ul \ulc4 Exercise M4.01
\f0\b0 \cf2 \ulnone \
	\'95 linear_models_ex_01.ipynb\
	\'95 My regression guesses:\
		weights = [45, 52, 45.5, 51, 50, 49, 48, 47, 46.5,  46]\
		intercepts = [-5000, -5900, -5000, -5900, -5800, -5600, -5500, -5300, -5200, -5500]\
	\'95 numpy ravel turns both pandas series and pandas dataframes into 1-dimensional arrays\
		
\f1 \cf2 np.ravel(true_values)
\f0 \cf2 \
\

\f3\b \cf2 \'a7 4.1.3 \cf2 \ul \ulc4 Linear regression using scikit-learn
\f0\b0 \cf2 \ulnone \
	
\f3\b \cf2 \'95 
\f0\b0 \cf2 linear_regression_in_sklearn.ipynb\
	\'95 Parametrization of a linear model\
		\'ac Varying parameters give different models, with varying accuracy\
	\'95 Brute-force approach\
		\'ac check an array of parameters and pick the best one\
	\'95 Closed-form solution\
		\'ac Best parameter values can be found by solving an equation\
		\'ac Avoids brute-force search\
	\'95 Instead, use sklearn.linear_model.LinearRegression to find the coefficients and intercepts\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 linear_regression = LinearRegression()\
linear_regression.fit(data, target)\
weight_flipper_length = linear_regression.coef_[0]\
intercept_body_mass = linear_regression.intercept_
\f0 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Mean squared error\
		\'ac Computes the goodness of fit of a model\
		\'ac Difficult to interpret, since value is unrelated to data\
			
\f1 \cf2 from sklearn.metrics import mean_squared_error\
			inferred_body_mass = linear_regression.predict(data)\
			model_error = mean_squared_error(target, inferred_body_mass)
\f0 \cf2 \
	\'95 Mean absolute error\
		\'ac Value is in units of the data\
		\'ac\'a0eg, MAE = 310.00\
			\uc0\u9674  on average the model made an error of \'b1 313 grams when predicting penguin body mass\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.2.1 \cf2 \ul \ulc4 Exercise M4.02
\f0\b0 \cf2 \ulnone \
	\'95 linear_models_ex_02.ipynb\
	\'95 Scikit-learn models require data in column format\
		\'ac DataFrames for data\
		\'ac Series for target\
		
\f1 \cf2 from sklearn.linear_model import LinearRegression\
		model = LinearRegression()\
		model.fit(pd.DataFrame(data), pd.Series(target))
\f0 \cf2 \
			\uc0\u9674  numpy array reshaped to 
\f1 \cf2 (100, 1)
\f0 \cf2  would also work, though\
	\'95 Metrics seem to handle basic arrays just fine\
		
\f1 \cf2 from sklearn.metrics import mean_squared_error\
		error = mean_squared_error(target, predictions)
\f0 \cf2 \
	\

\f3\b \cf2 \'a7 4.2.2 \cf2 \ul \ulc4 Linear regression for a non-linear features-target relationship
\f0\b0 \cf2 \ulnone \
	\'95 linear_regression_non_linear_link.ipynb\
	\'95 Linear models can be made more expressive by engineering additional features\
		\'ac enhances performance on non-linear data\
		\'ac Combine a non-linear feature engineering step followed by linear regression\
	\'95 numpy reshape method argument "-1"\
		\'ac\'a0means shape is automatically inferred from the length of data and the remaining dimensions\
		
\f1 \cf2 data = data.reshape((-1, 1))\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf2 			\uc0\u9674  will take the shape of a single column, as set by the second dimension\
			
\f1 \cf2 data.shape
\f0 \cf2  returns 
\f1 \cf2 (100, 1)
\f0 \cf2 , eg\
	
\f3\b \cf2 \'b6 3 approaches to addressing non-linearity
\f0\b0 \cf2 \
		\'95 Choose a model that can natively deal with non-linearity\
		\'95 Enginner a richer set of features\
			\'ac Use expert knowledge which can be directly used by the linear model\
		\'95 Use a "kernel" to have a locally-based decision function\
			\'ac instead of global linear decision function\
	
\f3\b \cf2 \'b6 Choose a model which handles non-linearity
\f0\b0 \cf2 \
		\'95 Decision tree regressor\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1800\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11040\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.tree import DecisionTreeRegressor\
\
tree = DecisionTreeRegressor(max_depth=3).fit(data, target)\
target_predicted = tree.predict(data)\
mse = mean_squared_error(target, target_predicted)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-20 at 7.05.08 PM.png \width8200 \height6100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	
\f3\b \cf2 \'b6 Create new features using expert knowledge
\f0\b0 \cf2 \
		\'95 In this example, we have a cubic and squared relationship\
			\'ac we engineered the data in the first place to have this...\
		\'95 Polynomial feature expansion\
			\'ac Create two new features\
				
\f1 \cf2 data ** 2\
				data ** 3
\f0 \cf2 \
		\'95 By adding non-linear features can overcome linearity limitations\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1920\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10940\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 data_expanded = np.concatenate([data, data ** 2, data ** 3], axis=1)\
data_expanded.shape\
linear_regression.fit(data_expanded, target)\
target_predicted = linear_regression.predict(data_expanded)\
mse = mean_squared_error(target, target_predicted)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-20 at 7.19.47 PM.png \width8120 \height6080 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
	
\f3\b \cf2 	\'95 
\f0\b0 \cf2 Polynomial Features\
			\'ac instead of generating them manually, like above, scikit-learn has a module\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2760\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10080\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.pipeline import make_pipeline\
from sklearn.preprocessing import PolynomialFeatures\
\
polynomial_regression = make_pipeline(\
    PolynomialFeatures(degree=3),\
    LinearRegression(),\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
	
\f3\b \cf2 \'b6 Use a "kernel"
\f0\b0 \cf2 \
		\'95 Weight is assigned to each sample\
			\'ac Instead of learning a weight per feature\
			\'ac Not all samples will be used\
			\'ac Support vector machine algorithm\
		\'95 Beyond the scope of this course\
			\'ac\'a0see SVMs\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.svm import SVR\
svr = SVR(kernel="poly", degree=3)\
svr.fit(data, target)\
target_predicted = svr.predict(data)\
mse = mean_squared_error(target, target_predicted)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\'ac Produces similar graph to the above\
		\'95 For datasets larger than 10,000\
			\'ac  computationally more efficient to use feature expansion\
			\'ac eg, PolynomialFeatures\
			\'ac or, non-linear transformers:	\
				\uc0\u9674  KBinsDiscretizer\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth3300\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth9540\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.preprocessing import KBinsDiscretizer\
binned_regression = make_pipeline(\
    KBinsDiscretizer(n_bins=8), LinearRegression(),\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 					\cf2 {{\NeXTGraphic Screen Shot 2022-04-20 at 7.43.59 PM.png \width8180 \height6100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
				\uc0\u9674  Nystroem\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth3320\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth9520\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.kernel_approximation import Nystroem\
\
nystroem_regression = make_pipeline(\
    Nystroem(n_components=5), LinearRegression(),\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 					
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-20 at 7.44.54 PM.png \width8180 \height6100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.2.3 \cf2 \ul \ulc4 Exercise M4.03
\f0\b0 \cf2 \ulc4 \
	\cf2 \ulnone \'95 linear_models_ex_03.ipynb\
	\'95\'a0Training linear regression on datasets with more than one feature\
	\'95 Cross-validation list of valid 
\f1 \cf2 scoring
\f0 \cf2  arguments:\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.metrics import SCORERS\
sorted(SCORERS.keys())
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95\'a0Cross-validate with MAE as the metric\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import cross_validate\
cv_results = cross_validate(model, data, target, cv=10, scoring="neg_mean_absolute_error", return_estimator=True)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  Don't forget to change 'test_score' to positive to get MAE\
	\'95\'a0Create a box plot with column names as plot labels\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 weights = pd.DataFrame([lin_reg.coef_ for lin_reg in cv_results['estimator']], columns=data.columns)\
\
import matplotlib.pyplot as plt\
color = \{"whiskers": "black", "medians": "black", "caps": "black"\}\
weights.plot.box(color=color, vert=False)\
_ = plt.title("Value of linear regression coefficients")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  NOTE\
				\'86 make sure 
\f1 \cf2 columns=data.columns
\f0 \cf2  when you create the dataframe\
				\'86 keep default 
\f1 \cf2 use_index=True
\f0 \cf2  for column names to display in plot\
	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.3.1 \cf2 \ul \ulc4 Intuitions on regularized linear models
\f0\b0 \cf2 \ulnone \
	\'95\'a0How to avoid overfitting\
		Linear models are simpler\
			So expect them to be less overfitting\
		Often underfit\
			Esp when # of input variables is small vs # of data points eg less than 10 features\
			the problem is then not linearly separable\
	\'95 But they can overfit...\
		Possible causes\
			n_samples << n_features, eg, genomics\
			many uninformative features\
	Example for linear regression\
		Housing\
			eg, birthday of the first owner of the house\
			the linear model could find spurious relationships, which then causes overfitting\
		Feature selection is critical\
	Regularization\
		Reduces overfitting\
		Eg: Linear Regression\
			one alternative is Ridge regression\
		Ridge Regression\
			pass a parameter alpha to tune it\
			control the strengths of the regularization\
			tries to pull the coefficients toward zero\
				if a large value will reduce error, it is left alone\
				but if it does not reduce error, it will pull it toward zero more relative to other features\
			Alpha\
				no good default value for alpha\
				alpha set to 0 is just normal linear regression\
		Recommendation is to always use Ridge with tuned alpha\
	Regularization Example\
		Predict y given x\
		Very small training set\
			5 points\
		Linear regression gives a straight line\
		However, if sample is selected at random, it might not be representative\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-21 at 6.39.13 PM.png \width3320 \height2660 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		So if data set is small and noisy, random choice of sample can result in overfitting\
			see how general shape of data trends along a different slope\
		Instead, choose another 5 points\
			creates diferent solutions significantly\
			very sensitive to selection\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-21 at 6.41.41 PM.png \width8580 \height4420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
		Bias-variance tradeoff\
			Linear Regression\
				High variance problem\
					slope is very sensitive to the particular selection of the training set\
				No bias\
					model is free to find the best fit for a given set\
\
			Ridge Regression\
				Bias\
					restraints prevent the model from moving too much\

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-21 at 6.49.44 PM.png \width6960 \height1940 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
				Alpha too small, no regularization going to increased alpha\
					Good tradeoff should be found\
					Too much alpha puts all slopes close to zero\
						Intercept is not altered\

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-21 at 6.52.53 PM.png \width8160 \height3780 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Alpha trade off\
			GridSearchCV\
				use a range of possible values\
				only uses positive quantities\
				if it's zero, it's regular regression\
				will internally find the best alpha\
			RidgeCV\
				much faster than GridSearch\
				almost as fast as fitting a single Ridge model\
				100x faster than GridSearch\
		Regularization in logistic regression\
			LogisticRegression(C=1) is regularized by default\
				in scikit-learn\
			C is like alpha, but opposite\
				large C=1000, weaker regularization\
				C=0.01 is strongly regularized\
			Two effects of regularization\
				small C, strong reg, the region where the model has low confidence is larger\
					large C, weak reg, region of low confidence is much smaller\
				orientation with respect to the groups can change significantly\
					small reg, the points close to the decision boundary stongly affect the slope\
					large reg, many more points are involved in affecting the position of the line\
						to make it move, you have to consider many more data points\
	Take Home\
		Can overfit when:\
			n_samples is too small and n_features too large\
			esp, with non-informative features\
		Regularization for regression\
			linear -> ridge\
			large alpha: strong reg\
		Regularization classification\
			logistic regression regularized by default in scikit-learn\
			small C: strong reg\
		\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.3.2 \cf2 \ul \ulc4 Regularization of linear regression model\
	
\f0\b0 \cf2 \ulnone \'95 linear_models_regularization.ipynb\
	
\f3\b \cf2 \'b6 Effect of regularization
\f0\b0 \cf2 \
		\'95 Again, using PolynomialFeatures to augment the feature space and check with cross-validation\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2080\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10780\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import cross_validate\
from sklearn.pipeline import make_pipeline\
from sklearn.preprocessing import PolynomialFeatures\
from sklearn.linear_model import LinearRegression\
\
linear_regression = make_pipeline(PolynomialFeatures(degree=2),\
                                  LinearRegression())\
cv_results = cross_validate(linear_regression, data, target,\
                            cv=10, scoring="neg_mean_squared_error",\
                            return_train_score=True,\
                            return_estimator=True)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				\uc0\u9674  Compare 'train_score' with 'test_score' to see performance gap\
					\'86 gap shows overfitting happening\
			\'ac Common risk of PolynomialFeatures is overfitting\
		\'95 Examine weights of the model to confirm overfitting\
			\'ac 
\f1 \cf2 get_feature_names_out
\f0 \cf2  will return column names\
			\'ac vast majority of features have no range of impact on pricing\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-21 at 7.34.55 PM.png \width10940 \height7540 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
		\'95 Using Ridge instead gives a much smaller differnce in 'train_score' and 'test_score'\
			\'ac Meaning less overfitting\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11020\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.linear_model import Ridge\
\
ridge = make_pipeline(PolynomialFeatures(degree=2),\
                      Ridge(alpha=100))\
cv_results = cross_validate(ridge, data, target,\
                            cv=10, scoring="neg_mean_squared_error",\
                            return_train_score=True,\
                            return_estimator=True)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-21 at 7.40.05 PM.png \width10920 \height7520 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
	\
				\'ac Ridge enforces all weights to have similar magnitude\
					\uc0\u9674  overall scale of weights has moved toward 0 compared to linear reg above\
\
	
\f3\b \cf2 \'b6 Feature scaling and regularization
\f0\b0 \cf2 	\
		\'95 Interaction of elements\
			\'ac Weights define link between:\
				\uc0\u9674  feature values\
				\uc0\u9674  predicted target\
			\'ac Regularization constrains:\
				\uc0\u9674  weights\
				\uc0\u9674  using alpha\
			\'ac Feature scaling effects:\
				\uc0\u9674  weights\
				\uc0\u9674  regularization\
		\'95 Scaling\
			\'ac two equally important features on same scale \
				\uc0\u9674  will be regularized similarly\
			\'ac different data scale example: age in years, annual revenue in dollars\
				\uc0\u9674  boost features with small scale\
				\uc0\u9674  reduce weights of high scale features\
			\'ac regularization forces weights closer together\
				\uc0\u9674  rescaling forces data values closer together\
		\'95\'a0Note:\
			\'ac some solvers using gradients expect rescaled data\
		\'95 Using scaling brings 'test_score' and 'train_score' closer together\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1960\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10880\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),Ridge(alpha=0.5))
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				\uc0\u9674  less overfitted\
				\uc0\u9674  also better 'test_score'\
		\'95 Features are closer and more equal in contribution\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-21 at 10.31.14 PM.png \width10940 \height7920 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
		\'95 Increasing alpha decreases weight values\
			\'ac Negative alpha enhances large weights and overfitting\
		\'95 Scaling categorical features \
			\'ac common to omit them from scaling\
			\'ac but, when imbalanced (more samples in one category than another)\
				\uc0\u9674  scaling can even out impact of regularization\
			\'ac rare categories are problematic\
	
\f3\b \cf2 \'b6 Fine tuning the regularization parameter
\f0\b0 \cf2 \
		\'95 Out-of-sample rule\
			\'ac Care must be taken with training/testing, so that a chosen alpha is tested independently\
			\'ac cross-validation functions \
		\'95 Nested cross validation\
			\'ac inner CV will search for the best alpha\
			\'ac outer CV will provide estimate of the testing score\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import numpy as np\
from sklearn.linear_model import RidgeCV\
from sklearn.model_selection import ShuffleSplit\
\
alphas = np.logspace(-2, 0, num=20)\
\
ridge = make_pipeline(PolynomialFeatures(degree=2), \
StandardScaler(),RidgeCV(alphas=alphas, store_cv_values=True))\
\
cv = ShuffleSplit(n_splits=5, random_state=1)\
\
cv_results = cross_validate(ridge, data, target,cv=cv, scoring="neg_mean_squared_error", return_train_score=True,\
return_estimator=True, n_jobs=2)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
			\'ac With optimal alpha, training and testing are very close together\
				\uc0\u9674  very litle overfitting\
\
		
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-22 at 8.57.46 PM.png \width9140 \height6100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
			\'ac Store the error found in cross-validation\
				
\f1 \cf2 store_cv_values=True
\f0 \cf2 \
			\'ac The graph shows the trade-off of too high vs too low alpha\
		\'95 Now check the best alpha for cross-validation over each of the 5 shuffle splits\
			\'ac The best alpha does wander somewhat between iterations\
			\'ac Common practice is to use the average value alpha\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.3.3 \cf2 \ul \ulc4 Exercise M4.04
\f0\b0 \cf2 \ulnone \
	\'95 linear_models_ex_04.ipynb\
	\'95 Create a regression dataset, where 2 out of 3 features are informative\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.datasets import make_regression\
\
data, target, coef = make_regression(\
    n_samples=2_000,\
    n_features=5,\
    n_informative=2,\
    shuffle=False,\
    coef=True,\
    random_state=0,\
    noise=30,\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Build a data set with two informative columns and three noise columns\
		\'ac Original coefficients:\
		
\f1 \cf2 Relevant feature #0     9.566665\
		Relevant feature #1    40.192077\
		Noisy feature #0        0.000000\
		Noisy feature #1        0.000000\
		Noisy feature #2        0.000000
\f0 \cf2 \
		\'ac then add two duplicates of the two columns\
		\'ac\'a0running linear regression on this produces massive coefficients for the informative columns\
		
\f1 \cf2 [ 1.31388260e+12 -1.59473665e+14 -1.99218750e-01 -1.68457031e-01\
  9.52148438e-02 -6.56941302e+11  4.12806902e+13 -6.56941302e+11\
  1.18192975e+14]
\f0 \cf2 \
			\uc0\u9674  finding coefficients involves inverting the matrix 
\f1 \cf2 np.dot(data.T, data)
\f0 \cf2 \
				\'86 which is not possible (or creates massive errors)\
		\'ac running ridge on it applies penalty to the weights\
			\uc0\u9674  matrix inverted is 
\f1 \cf2 np.dot(data.T, data) + alpha * I
\f0 \cf2 \
			\uc0\u9674  which is possible\
		\'ac coefficients end up 3 times smaller than original, because of info columns multiplied 3 times\
		
\f1 \cf2 [ 3.6313933  13.46802113 -0.20549345 -0.18929961  0.11117205  \
	3.6313933 13.46802113  3.6313933  13.46802113]\

\f0 \cf2 \
	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.4.1 \cf2 \ul \ulc4 Linear model for classificaion
\f0\b0 \cf2 \ulnone \
	\'95 linear_models_ex_04.ipynb\
	\'95 Penguin dataset\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-23 at 8.00.13 AM.png \width8080 \height11680 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
				\uc0\u9674  Classification problem\
				\uc0\u9674  Culmen Depth is not helpful\
	\'95 Logistic function used to model the probability\
		\'ac Logistic regression\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.pipeline import make_pipeline\
from sklearn.preprocessing import StandardScaler\
from sklearn.linear_model import LogisticRegression\
\
logistic_regression = make_pipeline(\
    StandardScaler(), LogisticRegression(penalty="none")\
)\
logistic_regression.fit(data_train, target_train)\
accuracy = logistic_regression.score(data_test, target_test)\
print(f"Accuracy on test set: \{accuracy:.3f\}")
\f0 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  No regularization used:\
				
\f1 \cf2 penalty='none'
\f0 \cf2 \
	\'95 With only two features, we can display a decision boundary\
		\'ac DecisionBoundaryDisplay is currently a proposed pull-request\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import seaborn as sns\
from helpers.plotting import DecisionBoundaryDisplay\
\
DecisionBoundaryDisplay.from_estimator(\
    logistic_regression, data_test, response_method="predict", cmap="RdBu_r", alpha=0.5\
)\
sns.scatterplot(\
    data=penguins_test, x=culmen_columns[0], y=culmen_columns[1],\
    hue=target_column, palette=["tab:red", "tab:blue"])\
_ = plt.title("Decision boundary of the trained\\n LogisticRegression")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-23 at 8.13.53 AM.png \width8140 \height6480 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs30 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
			\uc0\u9674  since the line is oblique (slanted), it means we used a combination of features\
			\uc0\u9674  Note that no regularization happened\
				
\f1 \cf2 penalty='none'
\f0 \cf2 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.4.2 \cf2 \ul \ulc4 Exercise M4.05
\f0\b0 \cf2 \ulnone \
	\'95 linear_models_ex_05.ipynb\
	\'95 Arguments for settinng regularization\
		\'ac Type of regularization\
			
\f1 \cf2 penalty
\f0 \cf2 \
				\'86 default: 
\f1 \cf2 l2
\f0 \cf2 \
		\'ac Strength of regularization\
			
\f1 \cf2 C
\f0 \cf2 \
				\'86 penalty='none' is equivalent to setting C to infinity\
				\'86 in example, low C, or high regularization, makes the culmin width weight 0\
					\'bb this shows in a graph as a ine parallel to width axis\
\

\f3\b \cf2 \'a7  4.4.3 \cf2 \ul \ulc4 Beyond linear separation in classificaiton
\f0\b0 \cf2 \ulnone \
	\'95 logistic_regression_non_linear.ipynb\
	\'95 NOTE: logistic regression is not a regression problem, it is a classification problem\
	\'95 Similar methods available to address non-linear classification boundaries\
		\'ac feature augmentation with expert knowledge\
		\'ac kernel-based method\
	\'95 Two different methods of generating generic example data sets\
		\'ac Make moons\
			\uc0\u9674  two crescent-shaped classes hooked together\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.datasets import make_moons\
\
feature_names = ["Feature #0", "Features #1"]\
target_name = "class"\
\
X, y = make_moons(n_samples=100, noise=0.13, random_state=42)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
		\'ac Make Gaussian quantiles\
			\uc0\u9674  center class 1 ringed by class 2 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.datasets import make_gaussian_quantiles\
\
feature_names = ["Feature #0", "Features #1"]\
target_name = "class"\
\
X, y = make_gaussian_quantiles(\
    n_samples=100, n_features=2, n_classes=2, random_state=42)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Radial basis function (RBF) kernel \
		\'ac\'a0along with support vector machine classifier\
		
\f1 \cf2 kernel_model = make_pipeline(StandardScaler(), SVC(kernel="rbf", \
										gamma=5))
\f0 \cf2 \
	\'95 Overfitting risk\
		\'ac adding flexibility to models to fit non-linear boundaries can increase overfitting\
		\'ac makes decision function more sensitive to noisy data points\
	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.4.1 \cf2 \ul \ulc4 Module 4 Quiz
\f0\b0 \cf2 \ulnone \
	\'95 Select only numerical columns from dataset\
		
\f1 \cf2 data = adult_census.select_dtypes(["integer", "floating"])
\f0 \cf2 \
	\'95 Import DummyClassifier\
		\'ac set as constant, with constant argument\
		\'ac set as most_frequent\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.dummy import DummyClassifier\
\
# dumdum1 = DummyClassifier(strategy="constant", constant=" >50K")\
\
dumdum2 = DummyClassifier(strategy="most_frequent")\
cv_results6 = cross_validate(dumdum2, data, target, cv=10)\
cv_results7 = cross_validate(dumdum2, data, target, cv=10, scoring="balanced_accuracy")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7  4.5.1 \cf2 \ul \ulc4 Wrap Up
\f0\b0 \cf2 \ulnone \
	\'95 Predictions of linear model depend on:\
		\'ac weighted sum of values of the input features\
		\'ac added to an intercept parameter\
	\'95 Fitting a linear model consists of:\
		\'ac adjusting weight coefficients\
		\'ac\'a0adjusting intercept\
			\uc0\u9674  minimize predicion errors\
	\'95 Scaling needed to bring features into same dynamic range\
	\'95 Regularization can reduce overfitting\
		\'ac weights are constrained to stay small\
	\'95 Regularization hyperparameter\
		\'ac tuned by CV for each new:\
			\uc0\u9674  problem\
			\uc0\u9674  dataset\
	\'95 Feature engineering required for using linear models on non-linear relations\
		\'ac otherwise underfitting occurs\
			\uc0\u9674  training score and test score are farther apart\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97 \cf2 \ul \ulc4 MODULE 5: DECISION TREE MODELS
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Intuitions on tree-based models
\f0\b0 \cf2 \ulnone \
	\'95 https://www.youtube.com/watch?v=1kIHC1O_drM\
	\'95 Oliver Grisel\
	\'95 Decision Trees\
		Often the best performing with tabular data\
	What is a decision tree\
		Income classification in the US\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.01.30 PM.png \width4940 \height3720 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			hierarchical decisions based on one variable at a time\
			reaching the leaf gives the answer\
	Growing a classification tree\
		Two groups of data points\
			predict their color\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.03.54 PM.png \width4000 \height3240 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Root node of the decision tree\
			The red line in the graph\
			partition data into everything on the left and everything on the right\
				left, predict bluc, right predict orange\
			Or, predict class probabilities\
				count numbers of blue and orange on left, divide the total number on the left\
				find probability of being blue or orange on the left\
		After the first split, new decision node\
			Horisontal split along x1 axis\
			Now 100% accurate for this split\
		Another node, with another horizontal split\
			Again, 100% accurate\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.08.45 PM.png \width8960 \height4040 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	Regression with a decision tree\
		Root node, decision node, split\
		Split on specific value of x\
			average value of y on the left\
			average value of x on the right\
			Constant piecewise is only loosly approximate\
			(trying a linear model would have underfit as well_\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.13.32 PM.png \width3840 \height2540 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Second split, two additional subgroups\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.14.34 PM.png \width3740 \height2360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Each new split, gives a new node\
			Green line starts to much more closely fitting the actual data\
	Tree Depth and Overfitting\
		Too deep will hurt generalization\
		Serious underfit on the left, both below and above\
			
\f1 \cf2 max_depth/max_leaf_nodes
\f0 \cf2  is too small\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.16.48 PM.png \width3660 \height3040 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
	Test and train error is approximately the same\
		no significant underfitting\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.17.01 PM.png \width3680 \height3020 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Eventually all the training points could be fit to their own leaf\
			but this causes overfit\
			Exactly zero training error\
			But the error for the grey data point prediction is worse than the example before\
				
\f1 \cf2 max_depth/max_leaf_nodes
\f0 \cf2  is too large\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.17.37 PM.png \width3740 \height3000 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
	Take home:\
		Sequence of simple decision rules\
			one feature and one threshold at a time\
		No scaling required for numerical features\
			good for tabular data\
			Decision exactly the same if scaled or not\
		
\f1 \cf2 max_depth
\f0 \cf2  controls trade-off between underfitting and overfitting\
		Mostly useful as a building block for ensemble model\
			Random Forests\
			Gradient Boosting Decision Trees\
				Not very good by themselves, though\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 5.1.1 \cf2 \ul \ulc4 Build a classification decision tree
\f0\b0 \cf2 \ulnone \
	\'95 trees_classification.ipynb\
	\'95 Linear classifier\
		\'ac defines linear separation to split classes\
		\'ac\'a0uses linear combination of input features\
			\uc0\u9674  displays as non-perpendicular to a specific axis\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 6.58.26 PM.png \width9920 \height5300 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	\'95 Decision trees are non-parametric models\
		\'ac no mathematical decision function\
		\'ac\'a0no weights or intercept\
	\'95\'a0Partition space by considering one feature at at time\
		
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 7.03.49 PM.png \width9500 \height5120 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\uc0\u9674  discards the feature culmin length\
				\'86 decision trees do not use a combination of features to make a split\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.tree import DecisionTreeClassifier\
\
tree = DecisionTreeClassifier(max_depth=1)\
tree.fit(data_train, target_train)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Tree structure\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.tree import plot_tree\
\
_, ax = plt.subplots(figsize=(8, 6))\
_ = plot_tree(tree, feature_names=culmen_columns,\
              class_names=tree.classes_, impurity=False, ax=ax)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\'ac 
\f1 \cf2 plt.subplots
\f0 \cf2  creates a figure and axis\
				\uc0\u9674  then pass the axis to the 
\f1 \cf2 plot_tree
\f0 \cf2  function for drawing\
	\'95 
\f3\b \cf2 criterion
\f0\b0 \cf2 \
		\'ac\'a0Partitions minimize the class diversity in each sub-partition\
		\'ac\'a0settable parameter\
		\'ac classifier will predict the most represented class within a partition\
			\uc0\u9674  eg, Adelie\
	\'95 Counts in the partition are also taken during training\
			\uc0\u9674  Data extraction and graph\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 y_pred_proba = tree.predict_proba(sample_2)\
y_proba_class_0 = pd.Series(y_pred_proba[0], index=tree.classes_)\
\
y_proba_class_0.plot.bar()\
plt.ylabel("Probability")\
_ = plt.title("Probability to belong to a penguin class")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  Calculations can also be done manually with the data counts from the table\
	\'95 Note: culmen length has been disregarded for the moment\
		\'ac\'a0not used in prediction\
	\'95 Scoring\
		
\f1 \cf2 test_score = tree.score(data_test, target_test)\
		print(f"Accuracy of the DecisionTreeClassifier: \{test_score:.2f\}")
\f0 \cf2 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 5.2.1 \cf2 \ul \ulc4 Decision tree for regression
\f0\b0 \cf2 \ulnone \
	\'95\'a0trees_regression.ipynb\
	\'95\'a0Synthetic dataset\
		\'ac min and max are the same as the actual flipper length column		\'ac numpy arange then fills in evenly spaced data, defaulted to increments of 1\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import numpy as np\
data_test = pd.DataFrame(np.arange(\
                         data_train[feature_name].min(),\
                         data_train[feature_name].max()),\
                         columns=[feature_name])
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95  Dashed line for 
\f1 \cf2 plt.plot
\f0 \cf2 \
		
\f1 \cf2 linestyle="--"
\f0 \cf2 \
\
	\'95 Non-parametric model\
		\'ac\'a0Do not make assumptions about how the data is distributed\
		\'ac\'a0Predictions are piecewise constant\
		\'ac Feature space divided into two partitions\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 9.33.37 PM.png \width8060 \height5060 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		\'ac Threshold for flipper length is 206.5mm\
			\uc0\u9674  vertical line\
		\'ac Predicted values on each side of the split are two constants\
			\uc0\u9674  3683.50 g and 5023.62 g\
			\uc0\u9674  values which correspond to mean values of training samples in each partition\
	\'95 Increased depth\
		\'ac\'a0Increases the number of partitions\
		\'ac And so increases the number of constant values that the tree is capable of predicting`\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-24 at 9.40.01 PM.png \width9080 \height5700 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 5.3.1 \cf2 \ul \ulc4 Importance of decision tree hyperparameters on generalization
\f0\b0 \cf2 \ulnone \
	\'95 trees_hyperparameters.ipynb\
	
\f3\b \cf2 \'b6 Effect of max_depth parameter
\f0\b0 \cf2 \
		\'95 Overfitting\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-25 at 5.32.31 PM.png \width9800 \height5240 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		\'95 Optimal??\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-25 at 5.33.42 PM.png \width9780 \height5220 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		\'95 Hyperparameter must be optimed for each application\
			\'ac no single value optimal for any dataset\
	
\f3\b \cf2 \'b6 Other hyperparemters in decision trees
\f0\b0 \cf2 \
		\'95 
\f1 \cf2 max_depth
\f0 \cf2  controls overall complexity of the tree\
			\'ac However, asymmetric development could also factor into overfitting\
		\'95\'a0Example creates a dataset where:\
			\'ac one portion is clearly segregated\
			\'ac one portion is mixed and need asymetric analysis\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-25 at 5.46.32 PM.png \width9760 \height5720 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-25 at 5.46.56 PM.png \width9800 \height6140 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
				\
				\'ac Fixing the 
\f1 \cf2 max_depth
\f0 \cf2  parameter would cut the tree horizontally somewhere\
					\uc0\u9674  even if more growth would be beneficial\
		\'95 Asymetric hyperparameters\
			
\f1 \cf2 min_samples_leaf
\f0 \cf2 			Will stop if next leaf split covers less than min
\f1 \cf2 \
			min_samples_split\
			max_leaf_nodes\
			min_impurity_decrease\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf2 \
				\'ac 
\f1 \cf2 min_samples_leaf = 60
\f0 \cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-25 at 5.54.53 PM.png \width8560 \height1540 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Wrap up
\f0\b0 \cf2 \ulnone \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95	are suited for both regression and classification problems;\uc0\u8232 \
	\'95	are non-parametric models;\uc0\u8232 \
	\'95	are not able to extrapolate;\uc0\u8232 \
	\'95	are sensitive to hyperparameter tuning.\cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97 \cf2 \ul \ulc4 MODULE 6: ENSEMBLE OF MODELS
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Contents
\f0\b0 \cf2 \ulnone \
	\'95 Bootstrapping\
		\'ac Bagging\
		\'ac Random forest\
	\'95 Boosting\
		\'ac AdaBoost\
		\'ac Gradient Boosting\
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Intuitions on ensemble models
\f0\b0 \cf2 \ulnone \
	\'95 https://www.youtube.com/watch?v=SnvdnIlOsHQ\
	\'95 Ensemble of tree-based models\
		Often over or underfit\
	\'95 Bagging\
		Random Forests\
	\'95 Boosting\
		Gradient Boosting\
	\'95 Bagging (Bootstrap Aggregating)\
		Two steps\
			Boosting\
			Aggregation\
\
		Take a random subset of the data points\
			The bootstrap samples overlap\
			each data in the training set will appear many times\
		1. Bootstrap\
			fit an independent model on each sample\
			we will have three different model for three samples\
			just a depth of one (in practice we use very deep trees)\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-26 at 6.04.37 PM.png \width9600 \height3360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
		2. Aggregation\
			first decison tree, predict on the majority side\
			eg: one orange and two blue votes\
			the resulting vote is of the ensesmble is blue\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-26 at 6.06.27 PM.png \width9440 \height3960 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
				
\f1 \cf2 BaggingClassifier\
				RandomForestClassifier
\f0 \cf2 \
		Typical decision is higher quality than the individual trees\
			even if individual trees overfit, average will overfit less\
		Eg 2: Regression\
			Non-linear relationship\
				Good amount of noise\
			1. Take multiple random subsets\
				enough that all the points will eventually be represented\
			2. Fit model perfectly\
				models will all overfit\
				but each one overfits differently\
			3. Average prediction does not overfit\

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-26 at 6.10.43 PM.png \width9180 \height4560 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
	\'95 Bagging vs Random Forests\
		Bagging is a general strategy for any base model\
			linear, trees, etc...\
		Random forests are bagged randomized decision trees\
			want to decorrelate the prediction errors of the individual trees\
			additional randomization step\
			when considering a split\
				instead of considering all the features to select the best feature to split on\
				we randomly take a subsample of the columns (features)\
				calculate best split for this random subsample\
				when decision trees are trained this way\
					we inject noisy constraint in the training procedure\
					extra randomization decorrelates the prediction error\
					make the averaging step of the bagging work better\
			uncorrelated errors make bagging work better\
	Let each deep tree overfit\
	Bagging ensemble averaging will not overfit\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.1.1 \cf2 \ul \ulc4 Introductory example to ensemble models
\f0\b0 \cf2 \ulnone \
	\'95\'a0ensemble_introduction.ipynb\
	\'95 Bagging ensemble method\
		\'ac 20 decision trees\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 %%time\
from sklearn.ensemble import BaggingRegressor\
\
base_estimator = DecisionTreeRegressor(random_state=0)\
bagging_regressor = BaggingRegressor(\
    base_estimator=base_estimator, n_estimators=20, random_state=0)\
\
cv_results = cross_validate(bagging_regressor, data, target, n_jobs=2)\
scores = cv_results["test_score"]\
\
print(f"R2 score obtained by cross-validation: "\
      f"\{scores.mean():.3f\} +/- \{scores.std():.3f\}")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  Better generalization performance than single, tuned tree and faster\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.1.2 \cf2 \ul \ulc4 Bagging
\f0\b0 \cf2 \ulc4 \
	
\f3\b \cf2 \ulnone \'95 
\f0\b0 \cf2 ensemble_bagging.ipynb\
	\'95 Bootstrap Aggregating\
		 \'ac 
\f3\b \cf2 bootstrap resampling
\f0\b0 \cf2 \
			\uc0\u9674  random sampling with replacement\
				\'86 meaning points can be sampled multiple times (vs. without replacement)\
	
\f3\b \cf2 \'b6 Bootstrap resampling
\f0\b0 \cf2 \
		\'95 Random sampling with replacement\
			\uc0\u9674  will have some data points multiple times, and some not at all\
			\uc0\u9674  resample will be the same size as the original\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 rng = np.random.RandomState(1)\
\
def bootstrap_sample(data, target):\
    # Indices corresponding to a sampling with replacement of the same sample\
    # size than the original data\
    bootstrap_indices = rng.choice(\
        np.arange(target.shape[0]), size=target.shape[0], replace=True,\
    )\
    # In pandas, we need to use `.iloc` to extract rows using an integer\
    # position index:\
    data_bootstrap = data.iloc[bootstrap_indices]\
    target_bootstrap = target.iloc[bootstrap_indices]\
    return data_bootstrap, target_bootstrap
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
				\'ac For example, from a dataset with 30 rows, the following indices are sampled:\
					
\f1 \cf2 Bootstrap indices: [20 21  6  2 12 27 21 11  7 13  8 11 \
										12 11 20  4  7  7 13  4 25 16 28 18
\f0 \cf2 \
				\'ac Number of unique samples:\
					~63.2% of the original data is present\
					~36.8% are repeated samples\
				\'ac Generating many datasets, each slightly different\
\
				
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-26 at 9.44.21 PM.png \width9120 \height5880 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	
\f3\b \cf2 \'b6 Aggregating
\f0\b0 \cf2 \
		\'95 Average the predictions\
			\'ac for a given test point:\
				\uc0\u9674  feed input feature to each model\
				\uc0\u9674  take all prediction per test point and average\
			\'ac\'a0less likely to overfit\
	
\f3\b \cf2 \'b6 Bagging in scikit-learn
\f0\b0 \cf2 \
		\'95\'a0
\f3\b \cf2 meta-estimator
\f0\b0 \cf2 \
			\'ac\'a0estimator that wraps another estimator\
			\'ac\'a0takes a base model, clones several times, trains on different resamples\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.ensemble import BaggingRegressor\
\
bagged_trees = BaggingRegressor(\
    base_estimator=DecisionTreeRegressor(max_depth=3),\
    n_estimators=100,\
)\
_ = bagged_trees.fit(data_train, target_train)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
			\'ac Internal models can be accessed in list after fitting\
				
\f1 \cf2 bagged_trees.estimators_
\f0 \cf2 \
			\'ac low 
\f1 \cf2 alpha
\f0 \cf2  opacity in the scatterplot shows the iterations\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-26 at 9.54.11 PM.png \width8160 \height5680 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 for tree_idx, tree in enumerate(bagged_trees.estimators_):\
    label = "Predictions of individual trees" if tree_idx == 0 else None\
    # we convert `data_test` into a NumPy array to avoid a warning raised in scikit-learn\
    tree_predictions = tree.predict(data_test.to_numpy())\
    plt.plot(data_test["Feature"], tree_predictions, linestyle="--", alpha=0.1,\
             color="tab:blue", label=label)\
\
sns.scatterplot(x=data_train["Feature"], y=target_train, color="black",\
                alpha=0.5)\
\
bagged_trees_predictions = bagged_trees.predict(data_test)\
plt.plot(data_test["Feature"], bagged_trees_predictions,\
         color="tab:orange", label="Predictions of ensemble")\
_ = plt.legend()
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
	
\f3\b \cf2 \'b6 Bagging complex pipelines
\f0\b0 \cf2 \
		\'95 Bagged polynomal regression pipeline\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.linear_model import Ridge\
from sklearn.preprocessing import PolynomialFeatures\
from sklearn.preprocessing import MinMaxScaler\
from sklearn.pipeline import make_pipeline\
\
\
polynomial_regressor = make_pipeline(\
    MinMaxScaler(),\
    PolynomialFeatures(degree=4),\
    Ridge(alpha=1e-10),\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\'ac Pipline processing:\
				\uc0\u9674  Scales the data to the 0-1 range\
					
\f1 \cf2 MinMaxScaler
\f0 \cf2 \
				\uc0\u9674  Extracts degree-4 polynomial features\
					\'86 features will all be in the 0-1 range\
						\'bb if x is 0-1, then x^n must als be in 0-1 for any n value\
				\uc0\u9674  
\f1 \cf2 alpha
\f0 \cf2  is made small, because bagging is expected to work well\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 bagging = BaggingRegressor(\
    base_estimator=polynomial_regressor,\
    n_estimators=100,\
    random_state=0,\
)\
_ = bagging.fit(data_train, target_train)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
			\'ac Pipeline is then passed to 
\f1 \cf2 BaggingRegressor\
		
\f0 \cf2 \'b6 Bagged polynomial regression looks better than the bagged trees\
			\'ac but we know that the example data was generated with a polynomial\
			\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.2.3 \cf2 \ul \ulc4 Exercise M6.01
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_ex_01.ipynb\
	\'95 Why this range of features??\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import RandomizedSearchCV\
param_dist = \{\
    'max_features': [.5, .8, 1.0],\
    'n_estimators': randint(10, 30),\
    'max_samples': [.5, .8, 1.0],\
    'base_estimator__max_depth': randint(3, 10)\}\
model_randomCV = RandomizedSearchCV(bagged_trees, param_distributions=param_dist,  n_iter=20, scoring="neg_mean_absolute_error")\

\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 Produces a random number somehow...\
		
\f1 \cf2 from scipy.stats import randint
\f0 \cf2 \
		
\f1 \cf2 randint(10, 30)
\f0 \cf2 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.2.3 \cf2 \ul \ulc4 Random forests
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_random_forest.ipynb\
	\'95 Modification of Bagging\
	\'95 Only decision trees for random forests\
		\'ac whereas bagging is for any classifier or regressor\
	\'95 The search for the best split is done only on a subset of the original features taken at random\
		\'ac different random subsets for each split node\
		\'ac injects additional randomization into learning to decorrelate prediction errors of individual trees\
			\uc0\u9674  Randomization occurs on both axes of the data matrix\
				\'86 bootstrapping samples at each tree in the forest\
				\'86 randomly selecting subset of features at each node\
	
\f3\b \cf2 \'b6 A look at random forests
\f0\b0 \cf2 \
		\'95\'a0
\f1 \cf2 OrdinalEncoder
\f0 \cf2  will work fine with trees, even with no true categorical rankings\
		\'95 Rare categories\
			\'ac\'a0specifically encode unknown categories at prediction time\
			\'ac\'a0otherwise, rare categories might only be present on the validation side of CV split\
				\uc0\u9674  
\f1 \cf2 OrdinalEncoder
\f0 \cf2  will raise an error\
		\'95 Alternate column transformer\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1920\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10920\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.preprocessing import OrdinalEncoder\
from sklearn.compose import make_column_transformer, make_column_selector\
\
categorical_encoder = OrdinalEncoder(\
    handle_unknown="use_encoded_value", unknown_value=-1\
)\
preprocessor = make_column_transformer(\
    (categorical_encoder, make_column_selector(dtype_include=object)),\
    remainder="passthrough"\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 Bagging example\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1980\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10860\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.ensemble import BaggingClassifier\
from sklearn.tree import DecisionTreeClassifier\
\
bagged_trees = make_pipeline(\
    preprocessor,\
    BaggingClassifier(\
        base_estimator=DecisionTreeClassifier(random_state=0),\
        n_estimators=50, n_jobs=2, random_state=0,\
    )\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
		\'95 Improvement of random forest over bagged trees\
			\'ac Possibly due to decorrelated prediction errors of individual trees\
			\'ac Makes averaging step more efficient at reducing overfitting\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11000\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.ensemble import RandomForestClassifier\
\
random_forest = make_pipeline(\
    preprocessor,\
    RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=0)\
)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 Then cross-validate to check it\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11000\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import cross_val_score\
scores_random_forest = cross_val_score(random_forest, data, target)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
	
\f3\b \cf2 \'b6 Details about default hyperparameters
\f0\b0 \cf2 \
		\'95\'a0Amount of randomness in each split can be controlled\
			
\f1 \cf2 max_features
\f0 \cf2 \
				\uc0\u9674  0.5 means 50% of the features are considered at each split\
				\uc0\u9674  1.0 would disable feature subsampling\
			
\f1 \cf2 RandomForestRegressor
\f0 \cf2 \
				\uc0\u9674  disables feature subsampling by default\
			
\f1 \cf2 RandomForestClassifier
\f0 \cf2 \
				
\f1 \cf2 max_features=np.sqrt(n_features)
\f0 \cf2 \
				\uc0\u9674  these reflect good practice from scientific literature\
		\'95 Tuning max_features\
			\'ac too much randomness can underfit base models\
			\'ac too few randomness gives more correlated prediction errors, and lessens averaging benefit\
		\'95 Bagging also has 
\f1 \cf2 max_features
\f0 \cf2  parameter\
			\'ac but because they're base model isn't always a tree\
			\'ac\'a0they can only randomly subsample features once before fitting each base model\
				\uc0\u9674  rather than each time when adding tree splits\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-04-27 at 7.43.59 PM.png \width11920 \height4740 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.2.4 \cf2 \ul \ulc4 Exercise M6.02
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_ex_02.ipynb\
	\'95 Extracting individual estimators from a forest\
		\'ac and plotting them on regularly spaced intervals\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import numpy as np\
\
data_range = pd.DataFrame(np.linspace(170, 235, num=300),\
                          columns=data.columns)\
tree_predictions = []\
for tree in random_forest['randomforestregressor'].estimators_:\
    # we convert `data_range` into a NumPy array to avoid a warning raised in scikit-learn\
    tree_predictions.append(tree.predict(data_range.to_numpy()))\
\
forest_predictions = random_forest.predict(data_range)\
\
\
\
import matplotlib.pyplot as plt\
import seaborn as sns\
\
sns.scatterplot(data=penguins, x=feature_name, y=target_name,\
                color="black", alpha=0.5)\
\
# plot tree predictions\
for tree_idx, predictions in enumerate(tree_predictions):\
    plt.plot(data_range[feature_name], predictions, label=f"Tree #\{tree_idx\}",\
             linestyle="--", alpha=0.8)\
\
plt.plot(data_range[feature_name], forest_predictions, label=f"Random forest")\
_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc="upper left")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.3.1 \cf2 \ul \ulc4 Intuitions on ensemble models
\f0\b0 \cf2 \ulnone \
	\'95 https://www.youtube.com/watch?v=k2ZCG1OLjVM&feature=youtu.be\
	\'95 Boosting and Gradient Boosting\
	\'95\'a0Boosting for classification\
		Eg\
			two features, classification task\
		instead of sampling randomly, consider full training set\
			set a shallow model over the whole set\
		prediction errors will guide us to reweight to correct for those samples in error\
	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.42.01 PM.png \width8460 \height4420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
		Again, another error appears, put more weight on it\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.42.19 PM.png \width10080 \height4240 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
		Next one \'9b\

\f4\fs24 \cf2 		\cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.43.09 PM.png \width11720 \height4000 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
		Trained iteratively, sequentially\
\
	\'95 Boosting for Regression\
		Underfitting regression model over the whole set\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.44.10 PM.png \width2800 \height2060 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Then try to predict better\
			More weight put on the samples farthest away from the prediction function\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.45.19 PM.png \width2700 \height1960 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Then combine prediction functions together\
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.46.29 PM.png \width2520 \height1960 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		Fit a new prediction with the altered weights\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.47.23 PM.png \width2620 \height2040 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		And then combine in the ensemble\

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.48.12 PM.png \width2620 \height2120 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	\'95 Boosting vs Gradient Boosting\
		AdaBoost\
			using reweighted learning procedure at each step\
			use any base model that accepts 
\f1 \cf2 sample_weight
\f0 \cf2 \
			not recommended in practice anymore\
		Gradient Boosting\
			for focusing on decsion trees\
			more computationally efficient, but specialized\
			Histogram Gradient Boosting\
				more complex to explain\
	\'95 Gradient Boosting and binned features\
		GradientBoostingClassifier\
			implementation of traditional (exact) method\
			fine for small data sets\
			too slow for samples > 10,000\
		HistGradientBoostingClassifier\
			preprocesses numerical features (256 levels, or binning)\
			efficient multi core implementation\
			computing histograms to approximate best split decision trees\
			much, much faster\
	\'95\'a0Take away\
		
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-28 at 7.54.47 PM.png \width11440 \height5300 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\
		\'ac In practice, Gradient Boosting works better than bagging or forests\
			because the trees are shallow, they tend to run faster\
			whereas the individual trees of bagging are more expensive\
			prediction accuracy is also slightly better\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.3.2 \cf2 \ul \ulc4 Adaptive Boosting (AdaBoost)
\f0\b0 \cf2 \ulnone \
	\'95\'a0ensemble_adaboost.ipynb\
	\'95 What does this do?????????\
		
\f1 \cf2 np.flatnonzero(
\f0 \cf2 \
	\'95 Returning a dataframe with only misclassified data\
		
\f1 \cf2 misclassified_samples_idx = np.flatnonzero(\
											target != target_predicted)\
		data_misclassified = data.iloc[misclassified_samples_idx]
\f0 \cf2 \
	\'95 Create a new classifier\
		\'ac discard all correctly classified samples\
		\'ac only consider misclassified\
			\uc0\u9674  misclassified assigned weight of 1, well classified a weight of 0\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 sample_weight = np.zeros_like(target, dtype=int)\
sample_weight[misclassified_samples_idx] = 1\
\
tree = DecisionTreeClassifier(max_depth=2, random_state=0)\
tree.fit(data, target, sample_weight=sample_weight)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
			\uc0\u9674  the new classification function now correctly classifies previous errors\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-29 at 5.05.54 PM.png \width6660 \height5200 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 {{\NeXTGraphic Screen Shot 2022-04-29 at 5.06.35 PM.png \width6640 \height4700 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs30 \cf2 \
\
	\'95 Check overlap in numpy arrays:\
		
\f1 \cf2 remaining_misclassified_samples_idx = \
			np.intersect1d(misclassified_samples_idx, \
						   newly_misclassified_samples_idx)
\f0 \cf2 \
\
	\'95 Weight predictions based on number of mistakes that each model is making\
		\'ac mistakes now being made on samples that were correct before\
		\'ac use classification error\
		\'ac Eg: classifier accuracy\
			\uc0\u9674  first one: 		94%\
			\uc0\u9674  second one: 	68%\
		\'ac use these percentages to weight the predictions of each model\
	\'95 Boosting stragegy\
		\'ac method to compute weights to be assigned to samples\
		\'ac method for assigning weight to each learner for predictions\
	\'95 AdaBoost\
		\'ac good demonstration of boosting algorithms\
		\'ac\'a0but not as efficient as gradient-boosting\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.3.3 \cf2 \ul \ulc4 Gradient-boosting decision tree (GBDT)
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_gradient_boosting.ipynb\
	\'95 Instead of assigning weights to samples\
		\'ac fits on the residuals error\
			\uc0\u9674  hence, "gradient"\
	\'95 Each new tree predicts the error from the previous model\
		\'ac rather than predicting the target\
	\'95 
\f3\b \cf2 Residuals
\f0\b0 \cf2 \
		\'ac error between predictions and the data\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-04-29 at 5.44.43 PM.png \width7560 \height5980 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		\'ac Now create a tree that uses 
\f1 \cf2 data
\f0 \cf2  to predict residuals instead of 
\f1 \cf2 target
\f0 \cf2 \
			
\f1 \cf2 residuals = target_train - target_train_predicted
\f0 \cf2 \
			
\f1 \cf2 tree_residuals = DecisionTreeRegressor(\
											max_depth=5, random_state=0)\
			tree_residuals.fit(data_train, residuals)
\f0 \cf2 \
\
				
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-29 at 5.49.29 PM.png \width9640 \height4920 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	\'95 Once the exact residual for a point can be accurately predicted\
		\'ac point can be predicted by adding the predictions of all the trees\
			\uc0\u9674  the second tree corrects the first, the third corrects the second\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.3.4 \cf2 \ul \ulc4 Exercise M6.03
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_ex_03.ipynb\
	\'95 Both models always improve with more trees\
		\'ac but plateau while training time keeps increasing\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-04-29 at 7.18.31 PM.png \width8080 \height6060 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	\'95 Gradient-boosting offers early-stopping option\
		\'ac algorithm uses out-of-sample set to compute generalization performance for each new tree\
		\'ac if performance stops improving, it stops adding trees\
		
\f1 \cf2 gradient_boosting2 = GradientBoostingRegressor(\
							 n_estimators=1000, n_iter_no_change=5)
\f0 \cf2 \
	\'95 Mean absolute error for the whole train/test split is lower than the CV error\
		\'ac The train/test has more overall training data than the CV\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.3.5 \cf2 \ul \ulc4 Speeding-up gradient-boosting
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_hist_gradient_boosting.ipynb\
	\'95 Histogram gradient boosting\
		\'ac uses reduced number of splits\
	\'95 Random forest efficiency\
		\'ac each tree can be fitted independently\
		\'ac algorithm scales efficiently with number of cores and number of samples\
	\'95 Gradient boosting is sequential\
		\'ac computationally expensive\
		\'ac most expensive part is search for best split\
			\uc0\u9674  brute-force approach\
			\uc0\u9674  all possible splits evaluated, best is chosen\
			\uc0\u9674  see "tree in depth" notebook\
	\'95 Accelerating gradient-boosting\
		\'ac\'a0reduce number of splits\
		\'ac reduces performance\
		\'ac but add more estimators to compensate\
	\'95 Discretizer\
		\'ac Reduce number of splits by binning data before gradient boosting\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import numpy as np\
from sklearn.preprocessing import KBinsDiscretizer\
\
discretizer = KBinsDiscretizer(\
    n_bins=256, encode="ordinal", strategy="quantile")\
data_trans = discretizer.fit_transform(data)\
data_trans
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  Transforms data into integral values\
			\uc0\u9674  value represents the bin index\
				
\f1 \cf2 n_bins=256
\f0 \cf2 \
					\'bb at most 256 bins\
			\uc0\u9674  warnings appear if too many bins make bins which are too small\
				\'86 too small bins are removed\
	\'95 Fit time is reduced, though performance is the same\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 print(f"Mean absolute error via cross-validation: "\
      f"\{-cv_results_gbdt['test_score'].mean():.3f\} +/- "\
      f"\{cv_results_gbdt['test_score'].std():.3f\} k$")\
print(f"Average fit time: "\
      f"\{cv_results_gbdt['fit_time'].mean():.3f\} seconds")\
print(f"Average score time: "\
      f"\{cv_results_gbdt['score_time'].mean():.3f\} seconds")
\f0 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 HistGradientBoosting is even more optimized for large datasets\
		\'ac Each feature is binned\
		\'ac especially advantageous over 10,000 samples\
		
\f1 \cf2 from sklearn.ensemble import HistGradientBoostingRegressor\
		histogram_gradient_boosting = HistGradientBoostingRegressor(\
    		max_iter=200, random_state=0, early_stopping=True)
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.4.1 \cf2 \ul \ulc4 Hyperparameter tuning
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_hyperparameters.ipynb\
	
\f3\b \cf2 \'b6 Random forest
\f0\b0 \cf2 \
		\'95 
\f1 \cf2 n_estimators
\f0 \cf2  is the main parameter to tune\
			\'ac\'a0More trees, better generalization performance\
			\'ac but will slow down fitting and prediction time\
				\uc0\u9674  goal is balance between performance and time\
		\'95 Depth of each tree in forest also an important hyperparameter\
			
\f1 \cf2 max_depth
\f0 \cf2 \
				\uc0\u9674  enforces more symmetric tree
\f1 \cf2 \
			max_leaf_nodes
\f0 \cf2 \
				\uc0\u9674  asymmetry ok\
		\'95 Deep trees are typical, since overfit on individuals is desired\
		\'95 Use a randomized search to check the relationship between n_estimators and max_leaf_nodes\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1740\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11100\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import pandas as pd\
from sklearn.model_selection import RandomizedSearchCV\
from sklearn.ensemble import RandomForestRegressor\
\
param_distributions = \{\
    "n_estimators": [1, 2, 5, 10, 20, 50, 100, 200, 500],\
    "max_leaf_nodes": [2, 5, 10, 20, 50, 100],\
\}\
search_cv = RandomizedSearchCV(\
    RandomForestRegressor(n_jobs=2), param_distributions=param_distributions,\
    scoring="neg_mean_absolute_error", n_iter=10, random_state=0, n_jobs=2,\
)\
\
cv_results = pd.DataFrame(search_cv.cv_results_)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				\'ac Results show better improvement from larger number of leaves\
					\uc0\u9674  especially once number of trees gets to at least 50 trees\
		\'95 Scoring the best model\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1760\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11080\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 search_cv.fit(data_train, target_train)\
error = -search_cv.score(data_test, target_test)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
	
\f3\b \cf2 \'b6 Gradient-boosting decision trees
\f0\b0 \cf2 \
		\'95 Parameters are coupled in gradient-boosting\
			\'ac cannot set parameters one after another\
		\'95 Most important are\
			
\f1 \cf2 n_estimators\
			learning_rate\
			max_depth
\f0 \cf2  or 
\f1 \cf2 max_leaf_nodes
\f0 \cf2 \
 		\'95 For max_depth, fitting full grown trees would be detrimental\
			\'ac\'a0low depth, 3-8 typically\
			\'ac or few leaves, 2
\fs24 \cf2 \super 3
\fs30 \cf2 \nosupersub  to 2
\fs24 \cf2 \super 8
\fs30 \cf2 \nosupersub  typically\
			\'ac\'a0weak learners at each step helps reduce overfitting\
		\'95 In turn, n_estimators should be increased if max_depth is low\
			\'ac as before, using early-stopping to prevent unnecessary iterations is best\
		\'95 learning_rate changes how many errors are corrected per each tree iteration\
			\'ac small rate only corrects few samples\
				\uc0\u9674  need more estimators overall\
				\uc0\u9674  takes more time\
			\'ac large learning rate can possibly create overfitted ensemble\
		\'95 Use Randomized Search to check a range of parameter options\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1860\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10980\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from scipy.stats import loguniform\
from sklearn.ensemble import GradientBoostingRegressor\
\
param_distributions = \{\
    "n_estimators": [1, 2, 5, 10, 20, 50, 100, 200, 500],\
    "max_leaf_nodes": [2, 5, 10, 20, 50, 100],\
    "learning_rate": loguniform(0.01, 1),\
\}\
search_cv = RandomizedSearchCV(\
    GradientBoostingRegressor(), param_distributions=param_distributions,\
    scoring="neg_mean_absolute_error", n_iter=20, random_state=0, n_jobs=2\
)\
cv_results = pd.DataFrame(search_cv.cv_results_)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				\uc0\u9674  learning rate needs to be at least > 0.1\
				\uc0\u9674 \'a0best ranked models need more trees, or leaves, for a smaller learning rate\
				\uc0\u9674  However, firm conclusions are difficult\
					\'86 Since the best value of a hyperparameter so heavily depends on the others\
		\'95 Note:  the best model fit with training data and scored with the test data will perform better\
			\'ac than the CV results, which uses a subset of the training set to train\
			\'ac because it uses a larger set for training\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 6.4.2 \cf2 \ul \ulc4 Exercise M6.04
\f0\b0 \cf2 \ulnone \
	\'95 ensemble_ex_04.ipynb\
	\'95\'a0Don't forget about KFold:\
		
\f1 \cf2 from sklearn.model_selection import KFold\
		Kfold = KFold(n_splits=5, shuffle=True, random_state=0)
\f0 \cf2 \
	\'95 Check the best resulting parameters and the number of trees from a GridSearch estimator\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clmgf \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clmrg \clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 for each in cv_results['estimator']:\
    print(each.best_params_)\
    print(each.best_estimator_.n_iter_)
\f0 \cf2 \cell 
\pard\intbl\itap1\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	\'95 I'm not sure what this means:\
		Inspect the results of the inner CV for each estimator of the outer CV. Aggregate the mean test score for each parameter combination and make a box plot of these scores.\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97 \cf2 \ul \ulc4 MODULE 7: EVALUATING MODEL PERFORMANCE
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Contents
\f0\b0 \cf2 \ulnone \
	\'95 When to use a specific cross-valdation or metric\
		\'ac eg, nested cv, optimization, regression, classification\
\

\f3\b \cf2 \'a7 7.1.1 \cf2 \ul \ulc4 Comparing model performance with a simple baseline
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_baseline.ipynb\
	\'95 ShuffleSplit \
		\'ac\'a0with 20% of data held for validation\
	\'95 Baseline\
		\'ac eg, Dummy Regressor\
			\uc0\u9674  always predicts the mean of the target column\
				
\f1 \cf2 strategy="mean"
\f0 \cf2 \
			\uc0\u9674  does not reference the data\
	\'95 Run both the DecisionTreeRegressor and the DummyRegressor through cross-validate\
		\'ac\'a0Store 'test_score' key in a pandas series\
	\'95 Plot these values\
		\'ac\'a0eg: decision tree is lower than dummy, but still considerable error\
	\'95 Analysis\
		\'ac\'a0we can show that a model accounting for features is more accurate\
		\'ac vs. just picking the average every time\
			\uc0\u9674  median is also an option, which performs better for a set with extreme outliers\
\

\f3\b \cf2 \'a7 7.1.2 \cf2 \ul \ulc4 Exercise M7.01
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_ex_02.ipynb\
	\'95\'a0Good separation between regression and dummy scores shows that regression is effective\
	\'95 Since classes are imbalanced, predicting most frequent will show better than realistic results\
	\'95 Stratified \
		\'ac\'a0randomly generates predictions based on the training set's distribution\
	\'95 Uniform\
		\'ac generates uniformly random predictions\
		\'ac\'a0for binary classification, CV will be 50% on average\
	\'95 
\f1 \cf2 permutation_test_score
\f0 \cf2 \
		\'ac instead of using a dummy classifier\
		\'ac compares CV of a model against the CV of the same model \
			\uc0\u9674  but trained on randomly permuted class labels\
		\'ac many ML estimators end up approximately behaving like most frequent Dummy Classifier\
			\uc0\u9674  ie, always predicting the majority class\
			\uc0\u9674  hence why this dummy is then substituted\
		\'ac 
\f3\b \cf2 chance level
\f0\b0 \cf2 \
			\uc0\u9674  for imbalanced classification problems\
			\uc0\u9674  
\f1 \cf2 most_frequent
\f0 \cf2  is sometimes called this, even though no "chance" is really at play\
		\'ac\'a0actually using 
\f1 \cf2 permutation_test_score
\f0 \cf2  is computationally intensive\
\

\f3\b \cf2 \'a7 7.2.1 \cf2 \ul \ulc4 Stratification
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_stratification.ipynb\
	\'95 Other options besides KFold and ShuffleSplit\
	\'95 Create a small random test set and run KFold as the splitter\
		\'ac\'a0Then use LogisticRegresion\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.preprocessing import StandardScaler\
from sklearn.linear_model import LogisticRegression\
from sklearn.pipeline import make_pipeline\
\
model = make_pipeline(StandardScaler(), LogisticRegression())\
\
import numpy as np\
from sklearn.model_selection import KFold\
\
data_random = np.random.randn(9, 1)\
cv = KFold(n_splits=3)\
for train_index, test_index in cv.split(data_random):\
    print("TRAIN:", train_index, "TEST:", test_index)\
\
from sklearn.model_selection import cross_validate\
\
cv = KFold(n_splits=3)\
results = cross_validate(model, data, target, cv=cv)\
test_score = results["test_score"]\
print(f"The average accuracy is "\
      f"\{test_score.mean():.3f\} +/- \{test_score.std():.3f\}")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
			\uc0\u9674  Train and test sets look like the following:\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10000\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 TRAIN: [3 4 5 6 7 8] TEST: [0 1 2]\
TRAIN: [0 1 2 6 7 8] TEST: [3 4 5]\
TRAIN: [0 1 2 3 4 5] TEST: [6 7 8]
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\uc0\u9674  And average accuracy ends up at 0\
			\uc0\u9674  Why?\
				\'86 The target vector is ordered\
					\'bb Our target column happens to have exactly three values\
				\'86 and KFold does not shuffle, it splits in order\
				
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-04-09 at 1.21.45 PM.png \width10420 \height2700 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 				\cf2 {{\NeXTGraphic Screen Shot 2022-05-07 at 7.31.38 PM.png \width6700 \height5140 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
	\'95 Add 
\f1 \cf2 shuffle
\f0 \cf2  in as a kwarg for KFold and it all works much better\
		
\f1 \cf2 cv = KFold(n_splits=3, shuffle=True, random_state=0)
\f0 \cf2 \
			\uc0\u9674  eg: Accuracy is 95%\
	\'95 Even better, use a 
\f1 \cf2 Stratified
\f0 \cf2  strategy for CV\
		
\f1 \cf2 from sklearn.model_selection import StratifiedKFold\
		cv = StratifiedKFold(n_splits=3)
\f0 \cf2 \
			\uc0\u9674  Rather than truly random shuffling\
			\uc0\u9674  eg: 96% accuracy\
			\uc0\u9674  
\f3\b \cf2 stratify
\f0\b0 \cf2 \
				\'86 for unequal numbers of each class\
				\'86 to preserve the original class frequencies\
				\'86 applied to the data split, according to class\
				\'86 ensures that each split gets an equal proportion of each class in the target column\
		\'ac KFold vs StratifiedKFold:\

\f4\fs24 \cf2 	\cf2 {{\NeXTGraphic Screen Shot 2022-05-07 at 7.54.29 PM.png \width6280 \height4860 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\cf2 {{\NeXTGraphic Screen Shot 2022-05-07 at 7.54.07 PM.png \width6240 \height4840 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 7.2.2 \cf2 \ul \ulc4 Sample grouping
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_grouping.ipynb\
	\'95 Examining sample groups some more\
		\'ac\'a0First, run with plain KFold\
		\'ac\'a0Then run with shuffled KFold\
			\uc0\u9674  this one has less variance (test scores are less spread out)\
	\'95 But there are two folds in the no-shufflle scores that are conspicuously lower\
	\'95 Looking at the data, there is a distinct pattern in the target column\
		\'ac looking at the documentation, it explains 13 individuals were responsible for writing test data\
		\'ac\'a0there are 14 very similar patterns of number sequences in the test data\
	\'95 Use those apparent patterns to chop up the data into chunks\
		\'ac\'a0and label each for a different writer\
	\'95 Supply this information to 
\f1 \cf2 GroupKFold
\f0 \cf2  through the cross-validation function\
		
\f1 \cf2 from sklearn.model_selection import GroupKFold\
		cv = GroupKFold()\
		test_score = cross_val_score(model, data, target, groups=groups, \
									cv=cv, n_jobs=2)
\f0 \cf2 \
	\'95 However, the score ends up being lower than unshuffled KFold\
		\'ac\'a00.931 +/- 0.026\
		\'ac\'a00.920 +/- 0.021\
			\uc0\u9674  But there was bias built into the test data that allowed "cheating"\
			\uc0\u9674  standard deviation is lower, thoughs\
\

\f3\b \cf2 \'a7 7.2.2 \cf2 \ul \ulc4 Non i.i.d. data
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_time.ipynb\
	\'95\'a0
\f3\b \cf2 Independent and Identically Distributed Random Variables
\f0\b0 \cf2 \
		\'ac i.i.d. data\
	\'95 Time series\
		\'ac samples depend on the past information\
		\'ac non-i.i.d.\
	\'95 Predictive model could not work on random data\
		\'ac can it work on stock price data\
		\'ac\'a0try to predict Chevron, using data from Exxon, ConocoPhillips and Valero\
	\'95\'a0Decision Tree Regressor was used\
		\'ac expected to overfit\
		\'ac not expected to generalize then\
	\'95 But regressor actually works very well for some reason\
		\'ac we try with CV\
		\'ac\'a0and also with R2, on a single test/train split, with 
\f1 \cf2 shuffle=True\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2040\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10800\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import train_test_split\
from sklearn.tree import DecisionTreeRegressor\
from sklearn.metrics import r2_score\
\
data, target = quotes.drop(columns=["Chevron"]), quotes["Chevron"]\
\
data_train, data_test, target_train, target_test = train_test_split(\
    data, target, shuffle=True, random_state=0)\
\
regressor = DecisionTreeRegressor()\
regressor.fit(data_train, target_train)\
target_predicted = regressor.predict(data_test)\
\
test_score = r2_score(target_test, target_predicted)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
	\'95 Our testing set closely mirrors the training set\
		\'ac so the model really just memorized the training set\
	\'95 Check if this is happening by not shuffling the data when doing the split\
		\'ac first 75% training, last 25% is testing\
		\'ac\'a0previously, not shuffling the data made it perform marginally worse\
			\uc0\u9674  Now, not shuffling causes the score to fail miserably\
			\uc0\u9674  the data to be predicted happens after the training\
				
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-05-07 at 10.25.25 PM.png \width9920 \height5160 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
	\'95 Other solutions for trying to predict:\
		\'ac Group samples into time blocks, eg by quarter\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1820\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11020\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import LeaveOneGroupOut\
\
groups = quotes.index.to_period("Q")\
cv = LeaveOneGroupOut()\
test_score = cross_val_score(regressor, data, target,\
                             cv=cv, groups=groups, n_jobs=2)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				\uc0\u9674  but this still fails\
		\'ac For forecasting, we shouldn't use training data ulterior to the testing data\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2020\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10820\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import TimeSeriesSplit\
\
cv = TimeSeriesSplit(n_splits=groups.nunique())\
test_score = cross_val_score(regressor, data, target,\
                             cv=cv, groups=groups, n_jobs=2)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
				\uc0\u9674  but this also fails\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 7.3.1 \cf2 \ul \ulc4 Nested cross-validation
\f0\b0 \cf2 \ulnone \
	\'95 cross_validation_nested.ipynb\
	\'95\'a0Using a single cross-validation for both\
		\'ac hyperparameter tuning and \
		\'ac estimating generalization performance\
	\'95 Problematic, because the evaluation will underestimate overfitting from the tuning process\
	\'95 Hyperparameter tuning is a form of ML itself\
		\'ac requires nested CV\
	\'95 GridSearchCV\
		\'ac has 
\f1 \cf2 .best_params_
\f0 \cf2  attribute\
		\'ac and 
\f1 \cf2 .best_score_
\f0 \cf2  attribute\
			\uc0\u9674  however, we used knowledge from the test sets to select the hyperparameters\
	\'95 Nested CV\
		\'ac\'a0Inner CV will only get the training set of the outer CV\
		\'ac\'a0keeps final testing scores independent\
	\'95 While the nested score might be close to the GridSearch score, it can be trusted\
		\'ac\'a0by doing multiple runs of both nested and bare GridSearch\
		\'ac we see overall, the basic GridSearch is optimistic compared to the nested CV\
\
\

\f3\b \cf2 \'a7 7.4.1 \cf2 \ul \ulc4 Classification
\f0\b0 \cf2 \ulnone \
	\'95 metrics_classification.ipynb\
	\'95 
\f3\b \cf2 Objective function
\f0\b0 \cf2 \
		\'ac ML operates by optimizing these functions\
		\'ac the objective function is typically decoupled from the evaluation metric\
			\uc0\u9674  the objective function serves as a proxy for the evaluation metric\
	\'95 Classification metrics\
		\'ac where target vector is categorical, rather than continuous\
	\'95 Bar graph the number of samples for each of the classes\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import matplotlib.pyplot as plt\
\
target.value_counts().plot.barh()\
plt.xlabel("Number of samples")\
_ = plt.title("Number of samples per classes present\\n in the target")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\'ac Eg: shows we have two, imbalanced classes\
	\'95\'a0Eg: only fits on a single split, rather than a proper CV, to focus on the metrics presentation\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11700\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth120\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.model_selection import train_test_split\
from sklearn.linear_model import LogisticRegression\
\
data_train, data_test, target_train, target_test = train_test_split(\
    data, target, shuffle=True, random_state=0, test_size=0.5)\
\
classifier = LogisticRegression()\
classifier.fit(data_train, target_train)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	
\f3\b \cf2 \'b6 Classifier predictions
\f0\b0 \cf2 \
		\'95\'a0Eg: then do a single prediction as well as predictions for the whole test set\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1920\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10920\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 new_donor = pd.DataFrame(\
    \{\
        "Recency": [6],\
        "Frequency": [2],\
        "Monetary": [1000],\
        "Time": [20],\
    \}\
)\
classifier.predict(new_donor)\
target_predicted = classifier.predict(data_test)\
target_predicted[:5]
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 	
\f3\b \cf2 \'b6 Accuracy as a baseline
\f0\b0 \cf2 \
		\'95 Compare predictions with the ground-truth\
			\'ac\'a0
\f3\b \cf2 ground-truth
\f0\b0 \cf2 \
				\uc0\u9674  the actual target vector from the test data\
				\uc0\u9674  
\f1 \cf2 target_test
\f0 \cf2 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1880\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10960\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 target_test == target_predicted
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 Calculate the accuracy\
			
\f1 \cf2 import numpy as np\
			np.mean(target_test == target_predicted)
\f0 \cf2 \
				OR:\
			
\f1 \cf2 from sklearn.metrics import accuracy_score\
			accuracy = accuracy_score(target_test, target_predicted)\
				
\f0 \cf2 OR:\
			
\f1 \cf2 classifier.score(data_test, target_test)
\f0 \cf2 \
	
\f3\b \cf2 \'b6 Confusion matrix and derived metrics
\f0\b0 \cf2 \
		\'95\'a0Finer granularity of the error in the example\
			\'ac either we predicted they would give blood when they didn't\
			\'ac or predicted they would not when they did\
			
\f1 \cf2 from sklearn.metrics import ConfusionMatrixDisplay\
			_ = ConfusionMatrixDisplay.from_estimator(\
										classifier, data_test, target_test)
\f0 \cf2 \
				\uc0\u9674  Confusion Matrix\
					TP		True positives\
					TN		True negatives\
					FN		False negatives\
					FP		False positives\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 						\cf2 {{\NeXTGraphic Screen Shot 2022-05-08 at 12.48.59 PM.png \width7540 \height4880 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
			\'ac With these splits, we can show generalization performance for certain settings\
				\uc0\u9674  eg, examine the set of people who gave when we predicted they would\
				\uc0\u9674  eg, examine everyone who gave, regardless of prediction\
		\'95 
\f3\b \cf2 Precision
\f0\b0 \cf2 \
			TP / (TP + FP)\
			\'ac the likelihood someone actually gave blood when they were predicted to do so\
		\'95
\f3\b \cf2  Recall
\f0\b0 \cf2 \
			TP / (TP + FN)\
			\'ac how well the classifier was able to correctly identify those who gave blood\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth2480\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10360\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.metrics import precision_score, recall_score\
\
precision = precision_score(target_test, \
            target_predicted, pos_label="donated")\
recall = recall_score(target_test, target_predicted,\
         pos_label="donated")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 Both Precision and Recall only focus on samples predicted to be positive\
			\'ac\'a0Accuracy takes both predictions into account\
		\'95 Compare with the confusion matrix\
			\'ac Left column\
				\uc0\u9674  more than half of donated predictions were correct\
				\uc0\u9674  Precision = .688\
			\'ac Right column\
				\uc0\u9674  a minority, but still a large number of donors mislabeled as not donated\
				\uc0\u9674  Recall = .124\
	
\f3\b \cf2 \'b6 The issue of class imbalance
\f0\b0 \cf2 \
		\'95 When classes are far out of balance, the accuracy score should not be used\
			\'ac a most frequent dummy classifier can actually beat the accuracy of our model\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11000\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.dummy import DummyClassifier\
dummy_classifier = DummyClassifier(strategy="most_frequent")\
dummy_classifier.fit(data_train, target_train)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 		\'95 Either use Precision and Recall, or use the balanced accuracy score instead\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1840\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth11000\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.metrics import balanced_accuracy_score\
balanced_accuracy = balanced_accuracy_score(target_test, target_predicted)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				\uc0\u9674  defined as the average recall obtained on each class\
	
\f3\b \cf2 \'b6 Evaluation and different probability thresholds
\f0\b0 \cf2 \
		\'95 Binary predictions of yes or no can be supplemented by probability ratios instead\
			
\f1 \cf2 pd.DataFrame(classifier.predict_proba(data_test),\
                                      columns=classifier.classes_)
\f0 \cf2 \
		\'95\'a0Get the labels out of a trained classifier for a pandas DataFrame\
			
\f1 \cf2 columns=classifier.classes_
\f0 \cf2 \
		\'95 Get the max value from each row of a DataFrame and its column name in a Series\
			
\f1 \cf2 target_proba_predicted.idxmax(axis=1)
\f0 \cf2 \
		\'95 Turn a Series into a numpy array\
			
\f1 \cf2 .to_numpy()
\f0 \cf2 \
		\'95 Check numpy array if all values are True\
			
\f1 \cf2 np.all(ndarray_name)\
		
\f0 \cf2 \'95 Check numpy array if any values are True\
			
\f1 \cf2 np.any(ndarray_name)
\f0 \cf2 \
		\'95 Binary predictions are based upon the default decision threshold of 0.5\
			\'ac but this might not lead to the optimal generalization performance of our classifier\
		\'95 Precision-Recall Curve\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1860\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10980\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.metrics import PrecisionRecallDisplay\
\
disp = PrecisionRecallDisplay.from_estimator(\
    classifier, data_test, target_test, pos_label='donated',\
    marker="+"\
)\
plt.legend(bbox_to_anchor=(1.05, 0.8), loc="upper left")\
_ = disp.ax_.set_title("Precision-recall curve")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-05-08 at 2.16.49 PM.png \width6440 \height5180 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
					\'86 Scikit-learn returns a display with all plotting elements\
					\'86 displays expose a matplotlib axis to add new elements\
						
\f1 \cf2 ax_
\f0 \cf2 \
		\'95 On this curve, each blue cross corresponds to a decision threshold probability\
			\'ac\'a0If we wanted to drive Recall to 1 (perfect) or the same for Precision\
				\uc0\u9674  then the other would go to 0\
			\'ac\'a0Looks like the goal would be to get ourselves as far away from the origin as possible\
				\uc0\u9674  ie, minimizing False Negatives (maximize Recall)\
				\uc0\u9674  and minimizing False Positives (maximize Precision)\
			\'ac But also depends of if FP or FN are more the more adverse outcome\
		\'95 Average Precision\
			\'ac AP\
			\'ac Area under the curve (AUC)\
			\'ac for ideal classifier, \
		\'95 Alternate metric for determining gen. performance by varying probability threshold\
			\'ac\'a0Focus on compromise of positive and negative class determination\
			\'ac 
\f3\b \cf2 Sensitivity
\f0\b0 \cf2 \
				TP / (TP + FN)\
				\uc0\u9674  same as Recall\
			\'ac 
\f3\b \cf2 Specificity\
				
\f0\b0 \cf2 TN / (TN + FP)\
				\uc0\u9674  changed from Precision by replacing with TN on top and bottom\
		\'95 Receiver Operating Characteristic Curve\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1880\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth10980\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth140\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.metrics import RocCurveDisplay\
\
disp = RocCurveDisplay.from_estimator(\
    classifier, data_test, target_test, pos_label='donated',\
    marker="+")\
disp = RocCurveDisplay.from_estimator(\
    dummy_classifier, data_test, target_test, pos_label='donated',\
    color="tab:orange", linestyle="--", ax=disp.ax_)\
plt.legend(bbox_to_anchor=(1.05, 0.8), loc="upper left")\
_ = disp.ax_.set_title("ROC AUC curve")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 				
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-05-08 at 3.06.11 PM.png \width6660 \height6180 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
		\'95 Overall generalization performance of the classifier\
			\'ac Area under the curve \
				\uc0\u9674  Lower bound is the most-frequent dummy classifier line\
				\uc0\u9674  Lower bound is 0.5\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 7.4.2 \cf2 \ul \ulc4 Exercise M7.02
\f0\b0 \cf2 \ulnone \
	\'95 metrics_ex_01.ipynb\
	\'95 Running a cross-validation with\
		\'ac a custom scorer, to evaluate Precision, using 'donated' as the positive label		\'ac using a list of scoring methods, then plotting the results\
	\'95 Another way to create a DataFrame\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 metrics = pd.DataFrame(\
[scores["test_accuracy"], scores["test_balanced_accuracy"]],\
index=["Accuracy", "Balanced accuracy"]).T
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 7.5.1 \cf2 \ul \ulc4 Regression
\f0\b0 \cf2 \ulnone \
	\'95 metrics_regression.ipynb\
	\'95 Optimization problem\
		\'ac\'a0some ML models are designed to minimize an error using the training set\
			\uc0\u9674  aka, 
\f3\b \cf2 loss function
\f0\b0 \cf2 \
	\'95 
\f3\b \cf2 Mean squared error\
		
\f1\b0 \cf2 mean_squared_error(target_test, target_predicted)
\f0 \cf2 \
			\uc0\u9674  one example of a loss function			\
	\'95 
\f3\b \cf2 R
\fs24 \cf2 \super 2
\f0\b0\fs30 \cf2 \nosupersub \
		
\f1 \cf2 regressor.score(data_test, target_test)
\f0 \cf2 \
			\uc0\u9674  coefficient of determination\
			\uc0\u9674  rescaled MSE\
				\'86 proportion of variance of the target\
					\'86 that is explained by the independent variables in the model\
			\uc0\u9674  the default score in scikit-learn\
				\
			\uc0\u9674  Best possible score is 1, no lower bound\
				\'86 However, model that predicts the expected value scores 0\
			\uc0\u9674  Gives insight into quality of the model's fit\
				\'86 But cannot be compared between datasets\
				\'86 No meaning relative to the original units of the target\
	\'95 
\f3\b \cf2 Mean absolute error
\f0\b0 \cf2 \
		
\f1 \cf2 mean_absolute_error(target_test, target_predicted)
\f0 \cf2 \
			\uc0\u9674  eg: our model is predicting, on average, 22.6 k$ away from the true price\
			\uc0\u9674  means can be overly impacted by large error\
	\'95 
\f3\b \cf2 Median absolute error
\f0\b0 \cf2 \
		
\f1 \cf2 median_absolute_error(target_test, target_predicted)
\f0 \cf2 \
			\uc0\u9674  Both Mean and Median have major limitation\
				\'86 incurring an error of 50 k$ on a 50 k$ house \
				\'86 has the same error impact as on a 500 k$ house\
	\'95 
\f3\b \cf2 Mean absolute percentage error
\f0\b0 \cf2 \
		
\f1 \cf2 mean_absolute_percentage_error(target_test, target_predicted)\
			
\f0 \cf2 \uc0\u9674  introduces relative scaling to address, eg, large error on a smaller house price
\f1 \cf2  
\f0 \cf2 \
	\'95 Plotting a graph\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import matplotlib.pyplot as plt\
import seaborn as sns\
\
predicted_actual = \{\
    "True values (k$)": target_test, "Predicted values (k$)": target_predicted\}\
predicted_actual = pd.DataFrame(predicted_actual)\
\
sns.scatterplot(data=predicted_actual,\
                x="True values (k$)", y="Predicted values (k$)",\
                color="black", alpha=0.5)\
plt.axline((0, 0), slope=1, label="Perfect fit")\
plt.axis('square')\
_ = plt.title("Regression using a model without \\ntarget transformation")
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
			
\f4\fs24 \cf2 {{\NeXTGraphic Screen Shot 2022-05-08 at 6.49.37 PM.png \width6860 \height6500 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
				\'86\'a0The plot give a visual way to check if the model makes consistent errors\
					\'bb eg, the model underestimates on large True Value prices\
					\'bb typically happens when target does not follow a normal distribution\
					\'bb try target transformation in these cases		\'95 
\f3\b \cf2 Target transformation
\f0\b0 \cf2 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 from sklearn.preprocessing import QuantileTransformer\
from sklearn.compose import TransformedTargetRegressor\
\
transformer = QuantileTransformer(\
    n_quantiles=900, output_distribution="normal")\
model_transformed_target = TransformedTargetRegressor(\
    regressor=regressor, transformer=transformer)\
model_transformed_target.fit(data_train, target_train)\
target_predicted = model_transformed_target.predict(data_test)\
\
predicted_actual = \{\
    "True values (k$)": target_test, "Predicted values (k$)": target_predicted\}\
predicted_actual = pd.DataFrame(predicted_actual)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\fs24 \cf2 			\cf2 {{\NeXTGraphic Screen Shot 2022-05-08 at 6.56.54 PM.png \width6880 \height6480 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f0\fs30 \cf2 \
				\'86 transformed target model performs much better for high values\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'a7 7.5.2 \cf2 \ul \ulc4 Exercise M7.03
\f0\b0 \cf2 \ulnone \
	\'95 metrics_ex_02.ipynb\
	\'95 Regression metrics within a cross-validation framework\
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Module 7 Quiz
\f0\b0 \cf2 \ulnone \
	\'95 Negative values in a series changed to 0s\
		
\f1 \cf2 accel = data['acceleration'].copy()\
		accel[accel < 0] = 0
\f0 \cf2 \
			Or:\
		
\f1 \cf2 accel.clip(lower=0)
\f0 \cf2 \
	\'95 Pandas index operations\
		\'ac Return a dataframe index\
			
\f1 \cf2 data.index
\f0 \cf2 \
				\uc0\u9674  object type:  
\f1 \cf2 pandas.core.indexes.datetimes.DatetimeIndex
\f0 \cf2 \
		\'ac Return a numpy array of dates\
			
\f1 \cf2 data.index.date
\f0 \cf2 \
		\'ac Return a numpy array of times\
			
\f1 \cf2 data.index.time
\f0 \cf2 \
		\'ac Change an index object into a numpy array\
			
\f1 \cf2 data.index.values
\f0 \cf2 \
		\'ac Return a numpy array of unique index values\
			
\f1 \cf2 data.index.unique().values
\f0 \cf2 \
	\'95 LeaveOneGroupOut generator\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clshdrawnil \clwWidth1220\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx2880
\clvertalc \clshdrawnil \clwWidth12240\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx5760
\clvertalc \clshdrawnil \clwWidth160\clftsWidth3 \clheight348 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 import numpy as np\
from sklearn.model_selection import LeaveOneGroupOut\
\
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\
y = np.array([1, 2, 1, 2])\
groups = np.array([1, 1, 2, 2])\
logo = LeaveOneGroupOut()\
\
# Returns all possible permutations of training and testing splits.\
for train_index, test_index in logo.split(X, y, groups):\
    print(train_index, test_index)
\f0 \cf2 \cell 
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 			\'ac For loop returns each possible permutations of testing and training sets\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \uc0\u55357 \u56446 \u55357 \u56446 \u55357 \u56446 
\f0 \cf2 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf2 \'97 \cf2 \ul \ulc4 CONCLUSION
\f0\b0 \cf2 \ulnone \
\

\f3\b \cf2 \'a7 \cf2 \ul \ulc4 Concluding remarks
\f0\b0 \cf2 \ulnone \
	\'95 https://www.youtube.com/watch?v=dqnPOlPYA4s\
	\'95 The big messages of the Mooc\
		\'ac The machine learning pipeline\
			\uc0\u9674  the learners, the predictive models are trained on the train set\
				\'86 different from the test set\
			\uc0\u9674  built from a data matrix\
				\'86 a give number of features for each observation\
			\uc0\u9674  transformations of the data\
				\'86 encoding categorical variables\
				\'86 using only information available at train time\
				\'86 scikit-learn pipeline object to facilitate this\
		\'ac Adapting model complexity to the data\
			\uc0\u9674  minimize the error on the test set\
				\'86 but train error can detect underfit\
					\'bb ie, models too simple for the data\
			\uc0\u9674  multiple hyper-parameters\
				\'86 they control model complexity\
				\'86 selecting them is important\
				\'86 scikit-learn: GridSearchCV, RandomSearchCV\
		\'ac Specific models\
			\uc0\u9674  understanding the models\
				\'86 know when they are suited to the data\
				\'86 debugging intuitions\
			\uc0\u9674  linear models\
				\'86 combining the values of features\
				\'86\'a0most useful for many features, or few observations\
			\uc0\u9674  tree-based models\
				\'86 series of binary choices (thresholds)\
				\'86 useful for tabular data, columns of different nature, eg, age, weight, sex\
				\'86 scikit-learn: HistGradientBoostingRegressor and Classifier\
		\'ac\'a0Learning more about scikit-learn\
			\uc0\u9674  documentation\
				\'86 rich, didactic, continuously improving\
				\'86 user guide\
			\uc0\u9674  stack overflow for questions\
			\uc0\u9674  we are an open-source community\
				\'86\'a0free, open, driven by community trying to be inclusive\
				\'86 help by training others, communication, advocacy\
	\'95 Impact on society\
		\'ac Validation and evaluation matter\
			\uc0\u9674  a measure of prediction accuracy is always an imperfect estimate of how well it generalizes\
				\'86 as you narrow down on a solution, spend increasingly more effort on validating it\
				\'86 do many splits in your cross-validation\
					\'bb despite cost in computation power\
		\'ac Machine learning is a small part of the problem most of the time\
			\uc0\u9674  how to approach the whole problem\
				\'86 what is the full value chain\
			\uc0\u9674  acquiring more/better data is often more important than using fancy models\
			\uc0\u9674  how are they being put into production, used routinely\
				\'86 technical debt\
					\'bb simpler models are easier to maintain, require less compute power\
				\'86 drifts in the data distribution as more is accumulated over time\
					\'bb continually checking model validation\
		\'ac\'a0Technical craft is not all\
			\uc0\u9674  methodological elements are not enough to always have a solid conclusion, statistically\
			\uc0\u9674  after running software, biggest challenges:\
				\'86 understanding the data\
				\'86 understanding data's shortcomings\
				\'86 what can and cannot be concluded\
			\uc0\u9674  automating machine learning  does not solve data science\
			\uc0\u9674  domain knowledge and critical thinking about the data are crucial\
		\'ac How the predicitons are used\
			\uc0\u9674  errors mean different things\
				\'86 operational risk\
					\'bb advertisement placement: errors are harmless\
					\'bb medicine: errors can kill\
				\'86 operational logic\
					\'bb better a false detection or a miss?\
					\'bb eg, detecting brain tumors\
						\'87 if sent to surgery: FP can be dangerous\
						\'87 however, sent to an MR scan: FN is the bigger threat, as scan is harmless\
			\uc0\u9674  predictions may modify how the system functions\
				\'86 eg, predicting who will benefit from a hospital stay may overcrowd the hospital\
		\'ac Choice of the output and the labeled dataset\
			\uc0\u9674  what we choose to predict is a very loaded choice\
			\uc0\u9674  interesting labels are often hard to get\
				\'86 we then emphasize easy ways of accumulating labels\
					\'bb but these come with biases\
			\uc0\u9674  our target value may just be a proxy of the real quantity of interest\
		\'ac Biases in the data\
			\uc0\u9674  the data may not reflect the ground truth\
				\'86 disease monitoring is a function of the testing policy\
					\'bb which may change over time, or uneven across the population\
					\'bb eg, wealthy people tend to have higher quality data\
			\uc0\u9674  the measured state of affaires may not be the desired one\
				\'86 eg, women are payed less than men\
				\'86 a learner will pick up and amplify inequalities\
		\'ac Prediction models vs Causal models\
			\uc0\u9674  eg, people that go to the hospital die more than people who don't\
				\'86 fallacy: comparing different populations\
			\uc0\u9674  eg, having blood pressure greater than a threshold triggers care which can be beneficial\
				\'86 fallacy: having above-threshold blood pressure is beneficial\
			\uc0\u9674  Pure predictive settings, information is beneficial for their predictions\
				\'86 but should not be trusted for designing interventions\
				\'86 interpretation is subject in caution\
		\'ac\'a0Societal impacts\
			\uc0\u9674  AI systems used for loans, jobs, medial treatment and law enforcement\
				\'86 shortcomings in the data and models will harm people				
\f1 \cf2 http://fairlearn.org
\f0 \cf2 \
					\'bb uses the same framework as scikit-learn\
			\uc0\u9674  ML can change:\
				\'86 decision logic\
				\'86\'a0power structure\
				\'86\'a0operational costs\
			\uc0\u9674  No solution will be purely technical\
\

\f3\b \cf2 \ul \ulc4 \'a7\cf2 \ulnone  \cf2 \ul \ulc4 Topics we have not covered
\f0\b0 \cf2 \ulnone \
Topics we have not covered\
	\'95 Unsupervised learning\
		\'ac Finding order and structure in the data, for instance to group samples, or to transform features\
		\'ac Particularly useful because it does not need labels\
		\'ac But given labels, supervised learning not unsupervised learning,\
			 is more likely to recover the link between data and labels\
	\'95 Model inspection\
		\'ac Understanding what drives a prediction\
		\'ac Useful for debugging, for reasoning on the system at hand\
		\'ac Requires a lot of nuance\
	\'95 Deep learning\
		\'ac Often not better than gradient boosting trees for classification or regression on tabular data\
		\'ac But more flexible: can work natively with tasks that involve variable length structures \
			in the input and output of the model (e.g. speech to text)\
		\'ac For images, text, voice: use pretrained models\
		\'ac Comes with great computational and human costs, as well as large maintenance costs\
		\'ac Not in scikit-learn: have a look at resources on pytorch and tensorflow to get started!\
					\
\
\
\
\
\
	\
\
\

\f3\b \cf2 Compare this to the quiz "plateau"
\f0\b0 \cf2 \
cross_validation_.ipynb\
	The average accuracy is 0.953 +/- 0.009\
	The average accuracy is 0.960 +/- 0.016\
\
Don't understand how lower bound of ROC-AUC is .5, when area under dummy is .5?\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
}